{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Polgraw Time Domain Fstatistic (TDFstat) pipeline: search for almost monochromatic gravitational wave signals This is the documentation of a gravitational-wave search pipeline of the Polgraw group. The pipeline's source code is available here . Pipeline flowchart Topics Input data generation F-statistic candidate signal search Coincidences between candidates False alarm probability of coincidences Followup of interesting outliers Sensitivity upper limits Pipeline: a minimal example Documents and publications Contributors In alphabetic order: Pia Astone Micha\u0142 Bejger Jan Bolek Pawe\u0142 Cieciel\u0105g Orest Dorosh Aleksander Garus Andrzej Kr\u00f3lak M\u00e1t\u00e9 Ferenc Nagy-Egri Maciej Pi\u0119tka Andrzej Pisarski Gevorg Poghosyan Magdalena Sieniawska Rafa\u0142 Skrzypiec","title":"Home"},{"location":"#polgraw-time-domain-fstatistic-tdfstat-pipeline-search-for-almost-monochromatic-gravitational-wave-signals","text":"This is the documentation of a gravitational-wave search pipeline of the Polgraw group. The pipeline's source code is available here .","title":"Polgraw Time Domain Fstatistic (TDFstat) pipeline: search for almost monochromatic gravitational wave signals"},{"location":"#pipeline-flowchart","text":"","title":"Pipeline flowchart"},{"location":"#topics","text":"Input data generation F-statistic candidate signal search Coincidences between candidates False alarm probability of coincidences Followup of interesting outliers Sensitivity upper limits Pipeline: a minimal example Documents and publications","title":"Topics"},{"location":"#contributors","text":"In alphabetic order: Pia Astone Micha\u0142 Bejger Jan Bolek Pawe\u0142 Cieciel\u0105g Orest Dorosh Aleksander Garus Andrzej Kr\u00f3lak M\u00e1t\u00e9 Ferenc Nagy-Egri Maciej Pi\u0119tka Andrzej Pisarski Gevorg Poghosyan Magdalena Sieniawska Rafa\u0142 Skrzypiec","title":"Contributors"},{"location":"articles/","text":"List of selected documents and publications Methods Data analysis of gravitational-wave signals from spinning neutron stars: The signal and its detection (arXiv) Data analysis of gravitational-wave signals from spinning neutron stars. II. Accuracy of estimation of parameters (arXiv) Data analysis of gravitational-wave signals from spinning neutron stars. III. Detection statistics and computational requirements (arXiv) Data analysis of gravitational-wave signals from spinning neutron stars. IV. An all-sky search (arXiv) Data analysis of gravitational-wave signals from spinning neutron stars. V. A narrow-band all-sky search (arXiv) Banks of templates for all-sky narrow-band searches of gravitational waves from spinning neutron stars (arXiv) Searching for gravitational waves from known pulsars at once and twice the spin frequency (arXiv) Virgo VSR1 data Implementation of an F-statistic all-sky search for continuous gravitational waves in Virgo VSR1 data (arXiv) Mock Data Challenge using the LIGO S6 data A comparison of methods for the detection of gravitational waves from unknown neutron stars (arXiv) LIGO O1 First search for gravitational waves from known pulsars with Advanced LIGO (arXiv) All-sky Search for Periodic Gravitational Waves in the O1 LIGO Data (arXiv) Full Band All-sky Search for Periodic Gravitational Waves in the O1 LIGO Data Resonant bar detectors All-sky search of NAUTILUS data (arXiv) All-sky upper limit for gravitational radiation from spinning neutron stars (arXiv) Monographs Gravitational-Wave Data Analysis. Formalism and Sample Applications: The Gaussian Case Analysis of Gravitational-Wave Data","title":"Documents and publications"},{"location":"articles/#list-of-selected-documents-and-publications","text":"","title":"List of selected documents and publications"},{"location":"articles/#methods","text":"Data analysis of gravitational-wave signals from spinning neutron stars: The signal and its detection (arXiv) Data analysis of gravitational-wave signals from spinning neutron stars. II. Accuracy of estimation of parameters (arXiv) Data analysis of gravitational-wave signals from spinning neutron stars. III. Detection statistics and computational requirements (arXiv) Data analysis of gravitational-wave signals from spinning neutron stars. IV. An all-sky search (arXiv) Data analysis of gravitational-wave signals from spinning neutron stars. V. A narrow-band all-sky search (arXiv) Banks of templates for all-sky narrow-band searches of gravitational waves from spinning neutron stars (arXiv) Searching for gravitational waves from known pulsars at once and twice the spin frequency (arXiv)","title":"Methods"},{"location":"articles/#virgo-vsr1-data","text":"Implementation of an F-statistic all-sky search for continuous gravitational waves in Virgo VSR1 data (arXiv)","title":"Virgo VSR1 data"},{"location":"articles/#mock-data-challenge-using-the-ligo-s6-data","text":"A comparison of methods for the detection of gravitational waves from unknown neutron stars (arXiv)","title":"Mock Data Challenge using the LIGO S6 data"},{"location":"articles/#ligo-o1","text":"First search for gravitational waves from known pulsars with Advanced LIGO (arXiv) All-sky Search for Periodic Gravitational Waves in the O1 LIGO Data (arXiv) Full Band All-sky Search for Periodic Gravitational Waves in the O1 LIGO Data","title":"LIGO O1"},{"location":"articles/#resonant-bar-detectors","text":"All-sky search of NAUTILUS data (arXiv) All-sky upper limit for gravitational radiation from spinning neutron stars (arXiv)","title":"Resonant bar detectors"},{"location":"articles/#monographs","text":"Gravitational-Wave Data Analysis. Formalism and Sample Applications: The Gaussian Case Analysis of Gravitational-Wave Data","title":"Monographs"},{"location":"coincidences/","text":"Coincidences between candidates In order to establish the probability of detection of a real gravitational wave, after finding the candidate signals with the F-statistic candidate signal search , the pipeline searches for coincidences between candidates found in different time frames. The coincidences code is available at github . To get it, run git clone https://github.com/mbejger/polgraw-allsky.git . Prerequisites The code is written in standard C . The only dependency is GNU Scientific Library (GSL) , used to manipulate the Fisher matrix (calculate the eigenvectors and eigenvalues). GNU struct dirent objects are used to read the directories. The idea behind coincidences After finding the candidate signals in different time frames ( search ), we want to confirm the existence of signals with the same parameters along the span of time segments. to further perform a validation search for high-coincidence, or otherwise interesting candidates (the followup , currently under construction). To do this, the candidates from a list of trigger files (time frames) are read, and for each trigger file a candidate is transformed to a well-defined time moment (frequency shifted to a reference frame), translated into linear (integer) coordinates, duplicates within each frame are removed, list of unique candidates from all the frames is created and sorted, duplicates are counted - these are the coincidences. TODO: describe cell shifts (16 different shifts in total: 0101, 0110, 0010 etc. in f, s, d, a directions) and scaling of cells (used to define the linear parameters for a given cell to subsequently compare the candidate values) Compilation Run make coincidences ; resulting binary is called coincidences . Modify the Makefile to fit your system. Full list of switches Type % ./coincidences --help to obtain the following description: Switch Description -data Data directory (default is ./candidates ) -output Output directory (default is ./coinc-results ) -shift Cell shifts in fsda directions (4 digit number, e.g. 0101 , default 0000 ) -scalef Cell scaling in f direction (a number, e.g. 32, default 1) -scales Cell scaling in s direction (a number, e.g. 8, default 1) -scaled Cell scaling in d direction (a number, e.g. 4, default 1) -scalea Cell scaling in a direction (a number, e.g. 4, default 1) -refr Reference frame number -fpo Reference band frequency fpo value -dt Data sampling time dt (default value: 0.5 ) -trigname Part of triggers' name (for identifying files) -refloc Location of the reference grid.bin and starting_date files -mincoin Minimal number of coincidences recorded -narrowdown Narrow-down the frequency band (range [0, 0.5] +- around center) -snrcutoff Signal-to-noise threshold cutoff (default value: 6) Also: --help This help Example Using the software injection added to 2-day Gaussian noise data segments (see minimal example of the pipeline ): % for s in {0..1}{0..1}{0..1}{0..1}; do ./coincidences -data ../../search/network/src-cpu -output . -shift $s -scalef 4 -scales 4 -scaled 4 -scalea 4 -refr 4 -dt 2 -trigname 1234_2 -refloc ../../testdata/2d_0.25/004 -nod 2 -fpo 308.859375 -snrcutoff 5; done 2>> summary This assumes that for the band bbbb=1234 bbbb=1234 and the sampling time dt=2\\ \\mathrm{s} dt=2\\ \\mathrm{s} the band frequency fpo=308.859375\\ \\mathrm{Hz} fpo=308.859375\\ \\mathrm{Hz} , because fpo = 10 + (1 - 2^{-5})\\cdot bbbb\\cdot \\frac{1}{2dt}\\ \\mathrm{[Hz]}. fpo = 10 + (1 - 2^{-5})\\cdot bbbb\\cdot \\frac{1}{2dt}\\ \\mathrm{[Hz]}. The reference grid file grid.bin , corresponding to the reference frame 004 ( -refr 4 ) is located at the -refloc location. Signal-to-noise ratio cutoff is set to 5 ( -snrcutoff 5 ). Some output is directed to stdin . For example, the output for shift 0000 is Number of days is 2 The SNR threshold cutoff is 5.000000000000, corresponding to F-statistic value of 14.500000000000 Reading the reference grid.bin at ../../testdata/2d_0.25/004 fftpad from the grid file: 1 Settings dt: 2.000000, oms: 3881.241374 Reference frame number: 4 Cell shifts in f, s, d, a directions: 0 0 0 0 Cell scaling in f, s, d, a directions: 4 4 4 4 Reading triggers... Frame 5: 1966/2040 Reading triggers... Frame 1: 2409/2483 Reading triggers... Frame 4: 2176/2384 Reading triggers... Frame 3: 2132/2247 Reading triggers... Frame 8: 2372/2408 Reading triggers... Frame 2: 2197/2249 Reading triggers... Frame 6: 2225/2305 Reading triggers... Frame 7: 2175/2226 Total number of candidates from all frames: 17652 The highest multiplicity coincidence for each shift is streamed to the summary file via the stderr ( 2>> summary ). From this list of 16 lines, the highest multiplicity coincidence with the highest signal-to-noise is selected ( sort -gk5 -gk10 summary | tail -1 ): 1234_2 1111 308.859375 8 5 9.95663703e-01 -1.10830358e-09 -1.12585347e-01 1.97463002e+00 1.246469e+01 5 2040 1987 1 2483 2419 4 2384 2193 3 2247 2137 8 2408 2363 2 2249 2172 6 2305 2220 7 2226 2191 6 2 8 3 5 This output contains the band_hemisphere identifier ( 1234\\_2 1234\\_2 ), the shift value ( 1111 1111 ), the fpo reference band frequency ( 308.859375\\ \\mathrm{Hz} 308.859375\\ \\mathrm{Hz} ), the number of triggers files read ( 8 8 ), the multiplicity of coincidence found ( N_{coin}=5 N_{coin}=5 ), arithmetic mean values of the frequency \\bar{f} \\bar{f} (range [0:2\\pi] [0:2\\pi] , corresponding to the width of the band), mean frequency derivative \\bar{s} \\bar{s} (spindown, in Hz/s Hz/s ), equatorial coordinate system sky positions \\bar{\\delta} \\bar{\\delta} ( [\\pi:-\\pi] [\\pi:-\\pi] ) and \\bar{\\alpha} \\bar{\\alpha} ( [0:2\\pi] [0:2\\pi] ), and the mean signal-to-noise ratio, \\widetilde{\\mathrm{snr}}=\\sqrt{\\sum_i \\mathrm{snr}_i^2} \\widetilde{\\mathrm{snr}}=\\sqrt{\\sum_i \\mathrm{snr}_i^2} ( 5 5 floating-point numbers), 8 triplets of the frame number, number of all candidates in the corresponding triggers file, and the number of unique candidates after sorting to unique cells ( 8\\times 3 8\\times 3 integers), frame numbers participating in the coincidence ( 5 5 integers). Coincidences larger or equal mincoin (default value 3 ) are recorded in binary files .coi , separately for each shift, in the -output directory. Each coincidence is a set of following numbers: N_{coin},\\quad\\bar{f},\\quad\\bar{s},\\quad\\bar{\\delta},\\quad\\bar{\\alpha},\\quad\\widetilde{\\mathrm{snr}},\\quad\\mathrm{fr}_{1},\\,\\dots\\,\\mathrm{fr}_{N_{coin}},\\quad\\mathrm{p}_{1},\\,\\dots\\,\\mathrm{p}_{N_{coin}} N_{coin},\\quad\\bar{f},\\quad\\bar{s},\\quad\\bar{\\delta},\\quad\\bar{\\alpha},\\quad\\widetilde{\\mathrm{snr}},\\quad\\mathrm{fr}_{1},\\,\\dots\\,\\mathrm{fr}_{N_{coin}},\\quad\\mathrm{p}_{1},\\,\\dots\\,\\mathrm{p}_{N_{coin}} where N_{coin} N_{coin} is the multiplicity of coincidence (written as one unsigned short int ), \\bar{f} \\bar{f} , \\bar{s} \\bar{s} , \\bar{\\delta} \\bar{\\delta} , \\bar{\\alpha} \\bar{\\alpha} and \\widetilde{\\mathrm{snr}} \\widetilde{\\mathrm{snr}} are the mean parameters of the signal ( 5\\times 5\\times float ), \\mathrm{fr}_{1},\\,\\dots\\,\\mathrm{fr}_{N_{coin}} \\mathrm{fr}_{1},\\,\\dots\\,\\mathrm{fr}_{N_{coin}} are the frame numbers ( N_{coin}\\times N_{coin}\\times unsigned short int ), \\mathrm{p}_{1},\\,\\dots\\,\\mathrm{p}_{N_{coin}} \\mathrm{p}_{1},\\,\\dots\\,\\mathrm{p}_{N_{coin}} are the positions of candidate signals that took part in the coincidences, in their corresponding trigger files ( N_{coin}\\times N_{coin}\\times int ) in order to recover the information about the original triggers for further studies. .coi files can be read with the auxilary read_coi code: % gcc -o read_coi read_coi.c -lm % ./read_coi # num_of_coincidences mean_val_of_pars (f, s, d, a), snr frame_num:trigger_num_in_trigger_file # (see http://mbejger.github.io/polgraw-allsky/coincidences for details) 5 9.956637e-01 -1.108304e-09 -1.125853e-01 1.974630e+00 1.246469e+01 6:777 2:708 8:968 3:701 5:603 3 9.972902e-01 -1.174760e-10 -1.235594e-01 1.968819e+00 9.154494e+00 8:983 3:693 1:669 where the pairs frame-number:candidate-position-in-trigger-file have been arranged for readability.","title":"Coincidences between candidates"},{"location":"coincidences/#coincidences-between-candidates","text":"In order to establish the probability of detection of a real gravitational wave, after finding the candidate signals with the F-statistic candidate signal search , the pipeline searches for coincidences between candidates found in different time frames. The coincidences code is available at github . To get it, run git clone https://github.com/mbejger/polgraw-allsky.git .","title":"Coincidences between candidates"},{"location":"coincidences/#prerequisites","text":"The code is written in standard C . The only dependency is GNU Scientific Library (GSL) , used to manipulate the Fisher matrix (calculate the eigenvectors and eigenvalues). GNU struct dirent objects are used to read the directories.","title":"Prerequisites"},{"location":"coincidences/#the-idea-behind-coincidences","text":"After finding the candidate signals in different time frames ( search ), we want to confirm the existence of signals with the same parameters along the span of time segments. to further perform a validation search for high-coincidence, or otherwise interesting candidates (the followup , currently under construction). To do this, the candidates from a list of trigger files (time frames) are read, and for each trigger file a candidate is transformed to a well-defined time moment (frequency shifted to a reference frame), translated into linear (integer) coordinates, duplicates within each frame are removed, list of unique candidates from all the frames is created and sorted, duplicates are counted - these are the coincidences. TODO: describe cell shifts (16 different shifts in total: 0101, 0110, 0010 etc. in f, s, d, a directions) and scaling of cells (used to define the linear parameters for a given cell to subsequently compare the candidate values)","title":"The idea behind coincidences"},{"location":"coincidences/#compilation","text":"Run make coincidences ; resulting binary is called coincidences . Modify the Makefile to fit your system.","title":"Compilation"},{"location":"coincidences/#full-list-of-switches","text":"Type % ./coincidences --help to obtain the following description: Switch Description -data Data directory (default is ./candidates ) -output Output directory (default is ./coinc-results ) -shift Cell shifts in fsda directions (4 digit number, e.g. 0101 , default 0000 ) -scalef Cell scaling in f direction (a number, e.g. 32, default 1) -scales Cell scaling in s direction (a number, e.g. 8, default 1) -scaled Cell scaling in d direction (a number, e.g. 4, default 1) -scalea Cell scaling in a direction (a number, e.g. 4, default 1) -refr Reference frame number -fpo Reference band frequency fpo value -dt Data sampling time dt (default value: 0.5 ) -trigname Part of triggers' name (for identifying files) -refloc Location of the reference grid.bin and starting_date files -mincoin Minimal number of coincidences recorded -narrowdown Narrow-down the frequency band (range [0, 0.5] +- around center) -snrcutoff Signal-to-noise threshold cutoff (default value: 6) Also: --help This help","title":"Full list of switches"},{"location":"coincidences/#example","text":"Using the software injection added to 2-day Gaussian noise data segments (see minimal example of the pipeline ): % for s in {0..1}{0..1}{0..1}{0..1}; do ./coincidences -data ../../search/network/src-cpu -output . -shift $s -scalef 4 -scales 4 -scaled 4 -scalea 4 -refr 4 -dt 2 -trigname 1234_2 -refloc ../../testdata/2d_0.25/004 -nod 2 -fpo 308.859375 -snrcutoff 5; done 2>> summary This assumes that for the band bbbb=1234 bbbb=1234 and the sampling time dt=2\\ \\mathrm{s} dt=2\\ \\mathrm{s} the band frequency fpo=308.859375\\ \\mathrm{Hz} fpo=308.859375\\ \\mathrm{Hz} , because fpo = 10 + (1 - 2^{-5})\\cdot bbbb\\cdot \\frac{1}{2dt}\\ \\mathrm{[Hz]}. fpo = 10 + (1 - 2^{-5})\\cdot bbbb\\cdot \\frac{1}{2dt}\\ \\mathrm{[Hz]}. The reference grid file grid.bin , corresponding to the reference frame 004 ( -refr 4 ) is located at the -refloc location. Signal-to-noise ratio cutoff is set to 5 ( -snrcutoff 5 ). Some output is directed to stdin . For example, the output for shift 0000 is Number of days is 2 The SNR threshold cutoff is 5.000000000000, corresponding to F-statistic value of 14.500000000000 Reading the reference grid.bin at ../../testdata/2d_0.25/004 fftpad from the grid file: 1 Settings dt: 2.000000, oms: 3881.241374 Reference frame number: 4 Cell shifts in f, s, d, a directions: 0 0 0 0 Cell scaling in f, s, d, a directions: 4 4 4 4 Reading triggers... Frame 5: 1966/2040 Reading triggers... Frame 1: 2409/2483 Reading triggers... Frame 4: 2176/2384 Reading triggers... Frame 3: 2132/2247 Reading triggers... Frame 8: 2372/2408 Reading triggers... Frame 2: 2197/2249 Reading triggers... Frame 6: 2225/2305 Reading triggers... Frame 7: 2175/2226 Total number of candidates from all frames: 17652 The highest multiplicity coincidence for each shift is streamed to the summary file via the stderr ( 2>> summary ). From this list of 16 lines, the highest multiplicity coincidence with the highest signal-to-noise is selected ( sort -gk5 -gk10 summary | tail -1 ): 1234_2 1111 308.859375 8 5 9.95663703e-01 -1.10830358e-09 -1.12585347e-01 1.97463002e+00 1.246469e+01 5 2040 1987 1 2483 2419 4 2384 2193 3 2247 2137 8 2408 2363 2 2249 2172 6 2305 2220 7 2226 2191 6 2 8 3 5 This output contains the band_hemisphere identifier ( 1234\\_2 1234\\_2 ), the shift value ( 1111 1111 ), the fpo reference band frequency ( 308.859375\\ \\mathrm{Hz} 308.859375\\ \\mathrm{Hz} ), the number of triggers files read ( 8 8 ), the multiplicity of coincidence found ( N_{coin}=5 N_{coin}=5 ), arithmetic mean values of the frequency \\bar{f} \\bar{f} (range [0:2\\pi] [0:2\\pi] , corresponding to the width of the band), mean frequency derivative \\bar{s} \\bar{s} (spindown, in Hz/s Hz/s ), equatorial coordinate system sky positions \\bar{\\delta} \\bar{\\delta} ( [\\pi:-\\pi] [\\pi:-\\pi] ) and \\bar{\\alpha} \\bar{\\alpha} ( [0:2\\pi] [0:2\\pi] ), and the mean signal-to-noise ratio, \\widetilde{\\mathrm{snr}}=\\sqrt{\\sum_i \\mathrm{snr}_i^2} \\widetilde{\\mathrm{snr}}=\\sqrt{\\sum_i \\mathrm{snr}_i^2} ( 5 5 floating-point numbers), 8 triplets of the frame number, number of all candidates in the corresponding triggers file, and the number of unique candidates after sorting to unique cells ( 8\\times 3 8\\times 3 integers), frame numbers participating in the coincidence ( 5 5 integers). Coincidences larger or equal mincoin (default value 3 ) are recorded in binary files .coi , separately for each shift, in the -output directory. Each coincidence is a set of following numbers: N_{coin},\\quad\\bar{f},\\quad\\bar{s},\\quad\\bar{\\delta},\\quad\\bar{\\alpha},\\quad\\widetilde{\\mathrm{snr}},\\quad\\mathrm{fr}_{1},\\,\\dots\\,\\mathrm{fr}_{N_{coin}},\\quad\\mathrm{p}_{1},\\,\\dots\\,\\mathrm{p}_{N_{coin}} N_{coin},\\quad\\bar{f},\\quad\\bar{s},\\quad\\bar{\\delta},\\quad\\bar{\\alpha},\\quad\\widetilde{\\mathrm{snr}},\\quad\\mathrm{fr}_{1},\\,\\dots\\,\\mathrm{fr}_{N_{coin}},\\quad\\mathrm{p}_{1},\\,\\dots\\,\\mathrm{p}_{N_{coin}} where N_{coin} N_{coin} is the multiplicity of coincidence (written as one unsigned short int ), \\bar{f} \\bar{f} , \\bar{s} \\bar{s} , \\bar{\\delta} \\bar{\\delta} , \\bar{\\alpha} \\bar{\\alpha} and \\widetilde{\\mathrm{snr}} \\widetilde{\\mathrm{snr}} are the mean parameters of the signal ( 5\\times 5\\times float ), \\mathrm{fr}_{1},\\,\\dots\\,\\mathrm{fr}_{N_{coin}} \\mathrm{fr}_{1},\\,\\dots\\,\\mathrm{fr}_{N_{coin}} are the frame numbers ( N_{coin}\\times N_{coin}\\times unsigned short int ), \\mathrm{p}_{1},\\,\\dots\\,\\mathrm{p}_{N_{coin}} \\mathrm{p}_{1},\\,\\dots\\,\\mathrm{p}_{N_{coin}} are the positions of candidate signals that took part in the coincidences, in their corresponding trigger files ( N_{coin}\\times N_{coin}\\times int ) in order to recover the information about the original triggers for further studies. .coi files can be read with the auxilary read_coi code: % gcc -o read_coi read_coi.c -lm % ./read_coi # num_of_coincidences mean_val_of_pars (f, s, d, a), snr frame_num:trigger_num_in_trigger_file # (see http://mbejger.github.io/polgraw-allsky/coincidences for details) 5 9.956637e-01 -1.108304e-09 -1.125853e-01 1.974630e+00 1.246469e+01 6:777 2:708 8:968 3:701 5:603 3 9.972902e-01 -1.174760e-10 -1.235594e-01 1.968819e+00 9.154494e+00 8:983 3:693 1:669 where the pairs frame-number:candidate-position-in-trigger-file have been arranged for readability.","title":"Example"},{"location":"fap_coincidences/","text":"False alarm probability of coincidences A general formula for probability that is used in the estimation of significance of the coincidences is implemented. The code is available at github . Run git clone https://github.com/mbejger/polgraw-allsky.git to get the repository. Prerequisites The code is written in standard C . The only dependency is GNU Scientific Library (GSL) , used to manipulate the Fisher matrix (calculate the eigenvectors and eigenvalues), the \\Gamma \\Gamma function and for the combinations. Theoretical description This description is a short Appendix version of Implementation of an F-statistic all-sky search for continuous gravitational waves in Virgo VSR1 data . For a given frequency band we analyze L L non-overlapping time segments: the search in the l l -th segment produces N_l N_l candidates. The size of the parameter space for each time segment is the same and it can be divided into the number N_{\\rm cell} N_{\\rm cell} of independent cells. The code tests the null hypothesis that the coincidences among candidates from L L segments are accidental. The probability for a candidate event to fall into any given coincidence cell is equal to 1/N_{\\rm cell} 1/N_{\\rm cell} . The probability \\epsilon_l \\epsilon_l that a given coincidence cell is populated with one or more candidate events is given by \\epsilon_l = 1 - \\Big(1 - \\frac{1}{N_{\\rm cell}}\\Big)^{N_l} \\epsilon_l = 1 - \\Big(1 - \\frac{1}{N_{\\rm cell}}\\Big)^{N_l} , and for independent candidates (one candidate in one cell) it is \\epsilon_l = \\frac{N_l}{N_{\\rm cell}} \\epsilon_l = \\frac{N_l}{N_{\\rm cell}} . For two or more candidates within a given cell we choose the one with the highest signal-to-noise ratio. The probability p_F(N_{\\rm cell}) p_F(N_{\\rm cell}) that any given coincidence cell out of the total of N_{\\rm cell} N_{\\rm cell} cells contains candidate events from C_{max} C_{max} or more distinct data segments is given by a generalized binomial distribution: \\begin{eqnarray} p_F(N_{\\rm cell}) &=& \\sum_{n=C_{max}}^{L} \\frac{1}{n!(L-n)!} \\times \\sum_{\\sigma\\in\\Pi(L)} \\epsilon_{\\sigma(1)}\\ldots\\epsilon_{\\sigma(n)}(1-\\epsilon_{\\sigma(n+1)})\\ldots(1-\\epsilon_{\\sigma(L)}), \\end{eqnarray} \\begin{eqnarray} p_F(N_{\\rm cell}) &=& \\sum_{n=C_{max}}^{L} \\frac{1}{n!(L-n)!} \\times \\sum_{\\sigma\\in\\Pi(L)} \\epsilon_{\\sigma(1)}\\ldots\\epsilon_{\\sigma(n)}(1-\\epsilon_{\\sigma(n+1)})\\ldots(1-\\epsilon_{\\sigma(L)}), \\end{eqnarray} where \\sum_{\\sigma \\in \\Pi(L)} \\sum_{\\sigma \\in \\Pi(L)} is the sum over all permutations of L L data sequences. Finally the probability P_F P_F that there is C_{max} C_{max} or more coincidences in one or more of the N_{\\rm cell} N_{\\rm cell} cells is \\begin{equation} P_F = 1 - \\left(1 - p_F(N_{\\rm cell})\\right)^{N_{\\rm cell}}. \\end{equation} \\begin{equation} P_F = 1 - \\left(1 - p_F(N_{\\rm cell})\\right)^{N_{\\rm cell}}. \\end{equation} In order to find coincidences the entire cell coincidence grid is shifted by half a cell width in all possible 2^4 = 16 2^4 = 16 combinations of the four parameter-space dimensions of (f, \\dot{f}, \\delta, \\alpha) (f, \\dot{f}, \\delta, \\alpha) , and coincidences are searched in all the 16 coincidence grids. It does not account for cases when candidate events are located on opposite sides of cell borders, edges, and corners. This leads to a higher number of accidental coincidences, and consequently it underestimates the false alarm probability. In the four dimension parameter space of (f, \\dot{f}, \\delta, \\alpha) (f, \\dot{f}, \\delta, \\alpha) the formula for the probability P_{F}^{\\rm shifts} P_{F}^{\\rm shifts} that there are C_{max} C_{max} or more independent coincidences in one or more of the N_{\\rm cell} N_{\\rm cell} cells in all 16 grid shifts is \\begin{eqnarray} \\label{eq:FAPs} P^{\\rm shifts}_F = 1 - \\bigg[ 1 - \\Big( 2^4 p_F(N_c) - \\Big( {4 \\choose 1} p_F(2 N_c) + {4 \\choose 2} p_F(2^2 N_c) + {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) \\\\ \\nonumber - \\Big({4 \\choose 2} p_F(2^2 N_c) + {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) - \\Big( {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) - {4 \\choose 4} p_F(2^4 N_c)\\Big) \\bigg]^{N_c}. \\end{eqnarray} \\begin{eqnarray} \\label{eq:FAPs} P^{\\rm shifts}_F = 1 - \\bigg[ 1 - \\Big( 2^4 p_F(N_c) - \\Big( {4 \\choose 1} p_F(2 N_c) + {4 \\choose 2} p_F(2^2 N_c) + {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) \\\\ \\nonumber - \\Big({4 \\choose 2} p_F(2^2 N_c) + {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) - \\Big( {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) - {4 \\choose 4} p_F(2^4 N_c)\\Big) \\bigg]^{N_c}. \\end{eqnarray} By choosing a certain false alarm probability P_F P_F , we can calculate the threshold number C_{\\mathrm{max}} C_{\\mathrm{max}} of coincidences. If we obtain more than C_{max} C_{max} coincidences, the null hypothesis that coincidences are accidental is rejected at the significance level of P_F P_F . Compilation Run make fap ; resulting binary is called fap (modify the Makefile to fit your system). Full list of switches To obtain the full list of options, type % ./fap --help Switch Description -band Band number -cellsize Cell size (default value: 4) -data Coincidence summary file -grid Grid matrix directory (default value: .) -dt Data sampling time dt (default value: 2) -threshold FAP threshold (default value: 0.1) -nod Number of days -vetofrac Vetoed fraction of the band (default value: 0) Also: --help This help Example Using the software injection added to 2-day Gaussian noise data segments (see minimal example of the pipeline ): % ./fap -nod 2 -band 1234 -data <(sort -gk5 -gk10 summary | tail -1) -grid ../../testdata/2d_0.25/004 -vetofrac 0.0 -cellsize 4 -threshold 1.0 or, with the auxilary fap.sh script, % band=1234; bash fap.sh <(sort -gk5 -gk10 summary | tail -1) <(echo $band 0.0) ../../testdata/2d_0.25/004 Number of days in the time segment nod equals 2, fraction of the band vetoed vetofrac is 0 (no lines, Gaussian data) and the cell size scalling factor cellsize is 4. Directory containing the grid matrix file grid.bin of the reference frame (in this case frame 004 ) should be given by the grid switch. The input data is the last line of a sorted summary file to select the shift giving the best coincidence with the highest signal-to-noise ratio: 1234_2 1111 308.859375 8 5 9.95663703e-01 -1.10830358e-09 -1.12585347e-01 1.97463002e+00 1.246469e+01 5 2040 1987 1 2483 2419 4 2384 2193 3 2247 2137 8 2408 2363 2 2249 2172 6 2305 2220 7 2226 2191 6 2 8 3 5 (see the coincidences section for details). Output % ./fap -nod 2 -band 1234 -data <(sort -gk5 -gk10 summary | tail -1) -grid ../../testdata/2d_0.25/004 -vetofrac 0.0 -cellsize 4 -threshold 1.0 is Number of days in time segments: 2 Input data: /dev/fd/63 Grid matrix data directory: ../../testdata/2d_0.25/004 Band number: 1234 (veto fraction: 0.000000) The reference frequency fpo: 308.859375 The data sampling time dt: 2.000000 FAP threshold: 1.000000 Cell size: 4 1234 3.088594e+02 3.091094e+02 7.665713e-08 5 17682 9.956637e-01 -1.108304e-09 -1.125853e-01 1.974630e+00 1.246469e+01 2 The last line (in case the probability PFshifts is lower than the threshold ) is printed to stderr . The meaning of this output is the following: #band f_min f_max PFshifts noc Nkall f s d a hemisphere 1234 3.088594e+02 3.091094e+02 7.665713e-08 5 17682 9.956637e-01 -1.108304e-09 -1.125853e-01 1.974630e+00 1.246469e+01 2","title":"False alarm probablity of conicidences"},{"location":"fap_coincidences/#false-alarm-probability-of-coincidences","text":"A general formula for probability that is used in the estimation of significance of the coincidences is implemented. The code is available at github . Run git clone https://github.com/mbejger/polgraw-allsky.git to get the repository.","title":"False alarm probability of coincidences"},{"location":"fap_coincidences/#prerequisites","text":"The code is written in standard C . The only dependency is GNU Scientific Library (GSL) , used to manipulate the Fisher matrix (calculate the eigenvectors and eigenvalues), the \\Gamma \\Gamma function and for the combinations.","title":"Prerequisites"},{"location":"fap_coincidences/#theoretical-description","text":"This description is a short Appendix version of Implementation of an F-statistic all-sky search for continuous gravitational waves in Virgo VSR1 data . For a given frequency band we analyze L L non-overlapping time segments: the search in the l l -th segment produces N_l N_l candidates. The size of the parameter space for each time segment is the same and it can be divided into the number N_{\\rm cell} N_{\\rm cell} of independent cells. The code tests the null hypothesis that the coincidences among candidates from L L segments are accidental. The probability for a candidate event to fall into any given coincidence cell is equal to 1/N_{\\rm cell} 1/N_{\\rm cell} . The probability \\epsilon_l \\epsilon_l that a given coincidence cell is populated with one or more candidate events is given by \\epsilon_l = 1 - \\Big(1 - \\frac{1}{N_{\\rm cell}}\\Big)^{N_l} \\epsilon_l = 1 - \\Big(1 - \\frac{1}{N_{\\rm cell}}\\Big)^{N_l} , and for independent candidates (one candidate in one cell) it is \\epsilon_l = \\frac{N_l}{N_{\\rm cell}} \\epsilon_l = \\frac{N_l}{N_{\\rm cell}} . For two or more candidates within a given cell we choose the one with the highest signal-to-noise ratio. The probability p_F(N_{\\rm cell}) p_F(N_{\\rm cell}) that any given coincidence cell out of the total of N_{\\rm cell} N_{\\rm cell} cells contains candidate events from C_{max} C_{max} or more distinct data segments is given by a generalized binomial distribution: \\begin{eqnarray} p_F(N_{\\rm cell}) &=& \\sum_{n=C_{max}}^{L} \\frac{1}{n!(L-n)!} \\times \\sum_{\\sigma\\in\\Pi(L)} \\epsilon_{\\sigma(1)}\\ldots\\epsilon_{\\sigma(n)}(1-\\epsilon_{\\sigma(n+1)})\\ldots(1-\\epsilon_{\\sigma(L)}), \\end{eqnarray} \\begin{eqnarray} p_F(N_{\\rm cell}) &=& \\sum_{n=C_{max}}^{L} \\frac{1}{n!(L-n)!} \\times \\sum_{\\sigma\\in\\Pi(L)} \\epsilon_{\\sigma(1)}\\ldots\\epsilon_{\\sigma(n)}(1-\\epsilon_{\\sigma(n+1)})\\ldots(1-\\epsilon_{\\sigma(L)}), \\end{eqnarray} where \\sum_{\\sigma \\in \\Pi(L)} \\sum_{\\sigma \\in \\Pi(L)} is the sum over all permutations of L L data sequences. Finally the probability P_F P_F that there is C_{max} C_{max} or more coincidences in one or more of the N_{\\rm cell} N_{\\rm cell} cells is \\begin{equation} P_F = 1 - \\left(1 - p_F(N_{\\rm cell})\\right)^{N_{\\rm cell}}. \\end{equation} \\begin{equation} P_F = 1 - \\left(1 - p_F(N_{\\rm cell})\\right)^{N_{\\rm cell}}. \\end{equation} In order to find coincidences the entire cell coincidence grid is shifted by half a cell width in all possible 2^4 = 16 2^4 = 16 combinations of the four parameter-space dimensions of (f, \\dot{f}, \\delta, \\alpha) (f, \\dot{f}, \\delta, \\alpha) , and coincidences are searched in all the 16 coincidence grids. It does not account for cases when candidate events are located on opposite sides of cell borders, edges, and corners. This leads to a higher number of accidental coincidences, and consequently it underestimates the false alarm probability. In the four dimension parameter space of (f, \\dot{f}, \\delta, \\alpha) (f, \\dot{f}, \\delta, \\alpha) the formula for the probability P_{F}^{\\rm shifts} P_{F}^{\\rm shifts} that there are C_{max} C_{max} or more independent coincidences in one or more of the N_{\\rm cell} N_{\\rm cell} cells in all 16 grid shifts is \\begin{eqnarray} \\label{eq:FAPs} P^{\\rm shifts}_F = 1 - \\bigg[ 1 - \\Big( 2^4 p_F(N_c) - \\Big( {4 \\choose 1} p_F(2 N_c) + {4 \\choose 2} p_F(2^2 N_c) + {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) \\\\ \\nonumber - \\Big({4 \\choose 2} p_F(2^2 N_c) + {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) - \\Big( {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) - {4 \\choose 4} p_F(2^4 N_c)\\Big) \\bigg]^{N_c}. \\end{eqnarray} \\begin{eqnarray} \\label{eq:FAPs} P^{\\rm shifts}_F = 1 - \\bigg[ 1 - \\Big( 2^4 p_F(N_c) - \\Big( {4 \\choose 1} p_F(2 N_c) + {4 \\choose 2} p_F(2^2 N_c) + {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) \\\\ \\nonumber - \\Big({4 \\choose 2} p_F(2^2 N_c) + {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) - \\Big( {4 \\choose 3} p_F(2^3 N_c) + {4 \\choose 4} p_F(2^4 N_c) \\Big) - {4 \\choose 4} p_F(2^4 N_c)\\Big) \\bigg]^{N_c}. \\end{eqnarray} By choosing a certain false alarm probability P_F P_F , we can calculate the threshold number C_{\\mathrm{max}} C_{\\mathrm{max}} of coincidences. If we obtain more than C_{max} C_{max} coincidences, the null hypothesis that coincidences are accidental is rejected at the significance level of P_F P_F .","title":"Theoretical description"},{"location":"fap_coincidences/#compilation","text":"Run make fap ; resulting binary is called fap (modify the Makefile to fit your system).","title":"Compilation"},{"location":"fap_coincidences/#full-list-of-switches","text":"To obtain the full list of options, type % ./fap --help Switch Description -band Band number -cellsize Cell size (default value: 4) -data Coincidence summary file -grid Grid matrix directory (default value: .) -dt Data sampling time dt (default value: 2) -threshold FAP threshold (default value: 0.1) -nod Number of days -vetofrac Vetoed fraction of the band (default value: 0) Also: --help This help","title":"Full list of switches"},{"location":"fap_coincidences/#example","text":"Using the software injection added to 2-day Gaussian noise data segments (see minimal example of the pipeline ): % ./fap -nod 2 -band 1234 -data <(sort -gk5 -gk10 summary | tail -1) -grid ../../testdata/2d_0.25/004 -vetofrac 0.0 -cellsize 4 -threshold 1.0 or, with the auxilary fap.sh script, % band=1234; bash fap.sh <(sort -gk5 -gk10 summary | tail -1) <(echo $band 0.0) ../../testdata/2d_0.25/004 Number of days in the time segment nod equals 2, fraction of the band vetoed vetofrac is 0 (no lines, Gaussian data) and the cell size scalling factor cellsize is 4. Directory containing the grid matrix file grid.bin of the reference frame (in this case frame 004 ) should be given by the grid switch. The input data is the last line of a sorted summary file to select the shift giving the best coincidence with the highest signal-to-noise ratio: 1234_2 1111 308.859375 8 5 9.95663703e-01 -1.10830358e-09 -1.12585347e-01 1.97463002e+00 1.246469e+01 5 2040 1987 1 2483 2419 4 2384 2193 3 2247 2137 8 2408 2363 2 2249 2172 6 2305 2220 7 2226 2191 6 2 8 3 5 (see the coincidences section for details).","title":"Example"},{"location":"fap_coincidences/#output","text":"% ./fap -nod 2 -band 1234 -data <(sort -gk5 -gk10 summary | tail -1) -grid ../../testdata/2d_0.25/004 -vetofrac 0.0 -cellsize 4 -threshold 1.0 is Number of days in time segments: 2 Input data: /dev/fd/63 Grid matrix data directory: ../../testdata/2d_0.25/004 Band number: 1234 (veto fraction: 0.000000) The reference frequency fpo: 308.859375 The data sampling time dt: 2.000000 FAP threshold: 1.000000 Cell size: 4 1234 3.088594e+02 3.091094e+02 7.665713e-08 5 17682 9.956637e-01 -1.108304e-09 -1.125853e-01 1.974630e+00 1.246469e+01 2 The last line (in case the probability PFshifts is lower than the threshold ) is printed to stderr . The meaning of this output is the following: #band f_min f_max PFshifts noc Nkall f s d a hemisphere 1234 3.088594e+02 3.091094e+02 7.665713e-08 5 17682 9.956637e-01 -1.108304e-09 -1.125853e-01 1.974630e+00 1.246469e+01 2","title":"Output"},{"location":"fisher/","text":"Fisher matrix calculation The Fisher matrix associated with the signal model and its inversion is calculated using this code . Prerequisites The code is written in standard C and it's mostly based on functions used in search . Arbitrary-precision interval arithmetic Arb library is used to invert the (usually) not-very-well posed Fisher matrix, so it has to be installed beforehand. Arb requires FLINT , MPFR , and either MPIR or GMP . Compilation Run make fisher in search/network/src-cpu - resulting binary is called fisher . Modify the Makefile (especially the variable ARB_DIR ) to fit your system. Full list of switches For the full list of options, type % ./fisher --help Switch Description -data Data directory (default is . ) -ident Frame number -band Band number -fpo Reference band frequency fpo value -dt Data sampling time dt (default value: 0.5 ) -usedet Use only detectors from string (default is use all available ) -addsig Add signal with parameters from <file> Also: --help This help Example Minimal call to fisher is as follows: % ./fisher -data 2d_0.25 -ident 001 -band 1234 -usedet H1 -dt 2 -nod 2 -addsig sigfile where data is the base directory of input data files (e.g., this Gaussian data ), Sampling time dt is 2 s 2 s , ident is the number of time frame to be analyzed ( 001 001 ), nod number of days is 2 2 , band is the number of the frequency band (see the input data structure for details). usedet switch to chose a detector (here H1 H1 ) addsig switch to chose a file with signal data The sigfile file consists of 8 numbers: frequency [radians, between 0 and \\pi \\pi ] above fpo spindown (frequency time derivative) [ \\mathrm{Hz/s} \\mathrm{Hz/s} ] declination [radians, between \\pi \\pi and -\\pi -\\pi ] right ascension [radians, between 0 and 2\\pi 2\\pi ] 4 amplitudes a_1, a_2, a_3, a_4 a_1, a_2, a_3, a_4 e.g., 1.431318175386891 -7.9539e-9 0.6363615896875658 4.396884357060633 7.764354801848407e-3 -1.422468474545797e-2 -1.559826840666228e-2 -8.623005535014139e-3 The amplitudes a_1, a_2, a_3, a_4 a_1, a_2, a_3, a_4 correspond to the signal amplitude model h = a_1 h_1 + a_2 h_2 + a_3 h_3 + a_4 h_4, h = a_1 h_1 + a_2 h_2 + a_3 h_3 + a_4 h_4, where h(t) = \\left(a_1 a(t) + a_2 b(t)\\right)\\cos(\\psi) + \\left(a_3 a(t) + a_4 b(t)\\right)sin(\\psi), h(t) = \\left(a_1 a(t) + a_2 b(t)\\right)\\cos(\\psi) + \\left(a_3 a(t) + a_4 b(t)\\right)sin(\\psi), with \\psi(f, \\dot{f}, \\delta, \\alpha, t) \\psi(f, \\dot{f}, \\delta, \\alpha, t) being the phase of the signal, and a a and b b the amplitude modulation functions (calculated in the modvir function). Example output Number of days is 2 Input data directory is 2d_0.25 Frame and band numbers are 1 and 1234 The reference frequency fpo is 308.859375 The data sampling time dt is 2.000000 Adding signal from 'sigfile' Settings - number of detectors: 1 Using H1 IFO as detector #0... 2d_0.25/001/H1/xdatc_001_1234.bin as input time series data Using 2d_0.25/001/H1/DetSSB.bin as detector H1 ephemerids... The Fisher matrix: 1.4602194451385117e+10 9.6224528459395950e+14 1.0472943290223141e+10 1.9109038902196317e+11 -5.6465717962684985e+06 -4.1172082782989331e+06 -2.9517981884375727e+06 7.0470722915177587e+06 9.6224528459395950e+14 6.7134859540063060e+19 6.5156368686556762e+14 1.1289208033400466e+16 -3.3340781322676825e+11 -2.4401458951787103e+11 -1.7381344007331134e+11 4.1673668536337537e+11 1.0472943290223141e+10 6.5156368686556762e+14 8.0545511145922174e+09 1.5540065972734366e+11 -4.6112260504154256e+06 -3.4059196396034071e+06 -2.3414307185722976e+06 5.7018637665277841e+06 1.9109038902196317e+11 1.1289208033400466e+16 1.5540065972734366e+11 3.1204237328935298e+12 -9.2852217833914995e+07 -6.9173973293523684e+07 -4.6213130625602841e+07 1.1410003283718885e+08 -5.6465717962684985e+06 -3.3340781322676825e+11 -4.6112260504154256e+06 -9.2852217833914995e+07 7.6814199126393523e+03 -1.6833193661163169e-03 0.0000000000000000e+00 0.0000000000000000e+00 -4.1172082782989331e+06 -2.4401458951787103e+11 -3.4059196396034071e+06 -6.9173973293523684e+07 -1.6833193661163169e-03 1.0351065428550139e+04 0.0000000000000000e+00 0.0000000000000000e+00 -2.9517981884375727e+06 -1.7381344007331134e+11 -2.3414307185722976e+06 -4.6213130625602841e+07 0.0000000000000000e+00 0.0000000000000000e+00 7.6814199126393523e+03 -1.6833193661163169e-03 7.0470722915177587e+06 4.1673668536337537e+11 5.7018637665277841e+06 1.1410003283718885e+08 0.0000000000000000e+00 0.0000000000000000e+00 -1.6833193661163169e-03 1.0351065428550139e+04 Inverting the Fisher matrix... Diagonal elements of the covariance matrix: 2.561275e-04 3.867944e-18 2.363103e-03 9.137957e-04 1.343874e+05 4.107046e+04 3.329715e+04 1.117611e+05","title":"Fisher matrix calculation"},{"location":"fisher/#fisher-matrix-calculation","text":"The Fisher matrix associated with the signal model and its inversion is calculated using this code .","title":"Fisher matrix calculation"},{"location":"fisher/#prerequisites","text":"The code is written in standard C and it's mostly based on functions used in search . Arbitrary-precision interval arithmetic Arb library is used to invert the (usually) not-very-well posed Fisher matrix, so it has to be installed beforehand. Arb requires FLINT , MPFR , and either MPIR or GMP .","title":"Prerequisites"},{"location":"fisher/#compilation","text":"Run make fisher in search/network/src-cpu - resulting binary is called fisher . Modify the Makefile (especially the variable ARB_DIR ) to fit your system.","title":"Compilation"},{"location":"fisher/#full-list-of-switches","text":"For the full list of options, type % ./fisher --help Switch Description -data Data directory (default is . ) -ident Frame number -band Band number -fpo Reference band frequency fpo value -dt Data sampling time dt (default value: 0.5 ) -usedet Use only detectors from string (default is use all available ) -addsig Add signal with parameters from <file> Also: --help This help","title":"Full list of switches"},{"location":"fisher/#example","text":"Minimal call to fisher is as follows: % ./fisher -data 2d_0.25 -ident 001 -band 1234 -usedet H1 -dt 2 -nod 2 -addsig sigfile where data is the base directory of input data files (e.g., this Gaussian data ), Sampling time dt is 2 s 2 s , ident is the number of time frame to be analyzed ( 001 001 ), nod number of days is 2 2 , band is the number of the frequency band (see the input data structure for details). usedet switch to chose a detector (here H1 H1 ) addsig switch to chose a file with signal data The sigfile file consists of 8 numbers: frequency [radians, between 0 and \\pi \\pi ] above fpo spindown (frequency time derivative) [ \\mathrm{Hz/s} \\mathrm{Hz/s} ] declination [radians, between \\pi \\pi and -\\pi -\\pi ] right ascension [radians, between 0 and 2\\pi 2\\pi ] 4 amplitudes a_1, a_2, a_3, a_4 a_1, a_2, a_3, a_4 e.g., 1.431318175386891 -7.9539e-9 0.6363615896875658 4.396884357060633 7.764354801848407e-3 -1.422468474545797e-2 -1.559826840666228e-2 -8.623005535014139e-3 The amplitudes a_1, a_2, a_3, a_4 a_1, a_2, a_3, a_4 correspond to the signal amplitude model h = a_1 h_1 + a_2 h_2 + a_3 h_3 + a_4 h_4, h = a_1 h_1 + a_2 h_2 + a_3 h_3 + a_4 h_4, where h(t) = \\left(a_1 a(t) + a_2 b(t)\\right)\\cos(\\psi) + \\left(a_3 a(t) + a_4 b(t)\\right)sin(\\psi), h(t) = \\left(a_1 a(t) + a_2 b(t)\\right)\\cos(\\psi) + \\left(a_3 a(t) + a_4 b(t)\\right)sin(\\psi), with \\psi(f, \\dot{f}, \\delta, \\alpha, t) \\psi(f, \\dot{f}, \\delta, \\alpha, t) being the phase of the signal, and a a and b b the amplitude modulation functions (calculated in the modvir function).","title":"Example"},{"location":"fisher/#example-output","text":"Number of days is 2 Input data directory is 2d_0.25 Frame and band numbers are 1 and 1234 The reference frequency fpo is 308.859375 The data sampling time dt is 2.000000 Adding signal from 'sigfile' Settings - number of detectors: 1 Using H1 IFO as detector #0... 2d_0.25/001/H1/xdatc_001_1234.bin as input time series data Using 2d_0.25/001/H1/DetSSB.bin as detector H1 ephemerids... The Fisher matrix: 1.4602194451385117e+10 9.6224528459395950e+14 1.0472943290223141e+10 1.9109038902196317e+11 -5.6465717962684985e+06 -4.1172082782989331e+06 -2.9517981884375727e+06 7.0470722915177587e+06 9.6224528459395950e+14 6.7134859540063060e+19 6.5156368686556762e+14 1.1289208033400466e+16 -3.3340781322676825e+11 -2.4401458951787103e+11 -1.7381344007331134e+11 4.1673668536337537e+11 1.0472943290223141e+10 6.5156368686556762e+14 8.0545511145922174e+09 1.5540065972734366e+11 -4.6112260504154256e+06 -3.4059196396034071e+06 -2.3414307185722976e+06 5.7018637665277841e+06 1.9109038902196317e+11 1.1289208033400466e+16 1.5540065972734366e+11 3.1204237328935298e+12 -9.2852217833914995e+07 -6.9173973293523684e+07 -4.6213130625602841e+07 1.1410003283718885e+08 -5.6465717962684985e+06 -3.3340781322676825e+11 -4.6112260504154256e+06 -9.2852217833914995e+07 7.6814199126393523e+03 -1.6833193661163169e-03 0.0000000000000000e+00 0.0000000000000000e+00 -4.1172082782989331e+06 -2.4401458951787103e+11 -3.4059196396034071e+06 -6.9173973293523684e+07 -1.6833193661163169e-03 1.0351065428550139e+04 0.0000000000000000e+00 0.0000000000000000e+00 -2.9517981884375727e+06 -1.7381344007331134e+11 -2.3414307185722976e+06 -4.6213130625602841e+07 0.0000000000000000e+00 0.0000000000000000e+00 7.6814199126393523e+03 -1.6833193661163169e-03 7.0470722915177587e+06 4.1673668536337537e+11 5.7018637665277841e+06 1.1410003283718885e+08 0.0000000000000000e+00 0.0000000000000000e+00 -1.6833193661163169e-03 1.0351065428550139e+04 Inverting the Fisher matrix... Diagonal elements of the covariance matrix: 2.561275e-04 3.867944e-18 2.363103e-03 9.137957e-04 1.343874e+05 4.107046e+04 3.329715e+04 1.117611e+05","title":"Example output"},{"location":"followup/","text":"Followup Currently work in progress; for the present state of the code see here . Introduction On the last stage we can precisely estimate signal parameters. At this stage we focus only on a very narrow space around a candidate. Our goal is to find the maximum of the F-statistic and the corresponding signal parameters. To increase the efficiency of the code, we use special libraries ( GNU Scientific Library (GSL) , YEPPP! ) and parallelisation ( OpenMP ). We already implemented few algorithms to do this: Simplex algorithm, Mesh Adaptive Direct Search - MADS and modified MADS (called 'inverted MADS'). Compilation To compile a code, go to the followup/src and run: make followup Switches Run ./followup --help Switch Description -d, -data Data directory (default is .) -o, -output Output directory (default is ./candidates) -i, -ident Frame number -b, -band Band number -l, -label Custom label for the input and output files -c, -cwd Change to directory [dir] -t, -threshold Threshold for the F-statistic (default is 20) -p, -fpo Reference band frequency fpo value -s, -dt Data sampling time dt (default value: 0.5) -u, -usedet Use only detectors from string (default is use all available) -y, -nod Number of days -x, -addsig Add signal with parameters from [file] -a, -candidates As a starting point in followup use parameters from [file] -r, -refr Reference frame number Also: --vetolines Veto known lines from files in data directory --simplex Direct search of maximum using Nelder-Mead (simplex) algorithm --mads Direct search of maximum using MADS algorithm --gauss Generate Gaussian noise instead of reading data. Amplitude and sigma of the noise declared in init.c --neigh Function neigh() generate area as % from initial value instead of taking it from grid.bin --naive Function naive() generate area as +/- points taking it from grid.bin and divide it into smaller grid. --onepoint Calculate Fstatistic only in one point taken from file with candidates (without generating any grid). --help This help By default code is searching maximum of the F-statistic on the optimal, 4-dimensional grid (parameters of the grid, like e.g. minimal match, ale defined inside the code, in followup.c ). Main idea was to search on a denser grid than in search code, but to focus only on a few points around candidate. Additionally, when the point (on the optimal grid) with the maximal value of the F-statistic is established, one can run one of the direct maximum search algorithm: Nelder-Mead ( --simplex switch) or MADS ( --mads switch), to determine parameters of the maximum more precisely. In the current version of the code, by using --mads switch, one will use modified, 'inverted' MADS. To skip calculations on the optimal grid and run one of the algorithms directly from the initial point, use --onepoint switch. Input data Files required to run the code: xdatc* , DetSSB.bin and grid.bin . Path to the directory is taken from -data switch; there directories named as iii/H1 , iii/L1 etc. are expected, where iii is a frame number taken from -ident switch. Initial point of the calculations (switch -candidates ) is required. Usually the best result from coincidences (previous step of the pipeline) is taken. Parameters of the candidate need to be written into the file, as: frequency spin-down declination right_ascension Minimal example how to run a code Run: LD_LIBRARY_PATH=../../search/network/src-cpu/lib/yeppp-1.0.0/binaries/linux/x86_64 ./followup -data data_path/ -band 0666 -dt 16 -candidates path_to_candidate/candidate.txt -ident 001 -nod 6 --mads> output.txt As a result (last line in the output file) one can get parameters of the found maximum: frequency spin-down declination right_ascension F-statistic_value SNR","title":"Followup of interesting outliers"},{"location":"followup/#followup","text":"Currently work in progress; for the present state of the code see here .","title":"Followup"},{"location":"followup/#introduction","text":"On the last stage we can precisely estimate signal parameters. At this stage we focus only on a very narrow space around a candidate. Our goal is to find the maximum of the F-statistic and the corresponding signal parameters. To increase the efficiency of the code, we use special libraries ( GNU Scientific Library (GSL) , YEPPP! ) and parallelisation ( OpenMP ). We already implemented few algorithms to do this: Simplex algorithm, Mesh Adaptive Direct Search - MADS and modified MADS (called 'inverted MADS').","title":"Introduction"},{"location":"followup/#compilation","text":"To compile a code, go to the followup/src and run: make followup","title":"Compilation"},{"location":"followup/#switches","text":"Run ./followup --help Switch Description -d, -data Data directory (default is .) -o, -output Output directory (default is ./candidates) -i, -ident Frame number -b, -band Band number -l, -label Custom label for the input and output files -c, -cwd Change to directory [dir] -t, -threshold Threshold for the F-statistic (default is 20) -p, -fpo Reference band frequency fpo value -s, -dt Data sampling time dt (default value: 0.5) -u, -usedet Use only detectors from string (default is use all available) -y, -nod Number of days -x, -addsig Add signal with parameters from [file] -a, -candidates As a starting point in followup use parameters from [file] -r, -refr Reference frame number Also: --vetolines Veto known lines from files in data directory --simplex Direct search of maximum using Nelder-Mead (simplex) algorithm --mads Direct search of maximum using MADS algorithm --gauss Generate Gaussian noise instead of reading data. Amplitude and sigma of the noise declared in init.c --neigh Function neigh() generate area as % from initial value instead of taking it from grid.bin --naive Function naive() generate area as +/- points taking it from grid.bin and divide it into smaller grid. --onepoint Calculate Fstatistic only in one point taken from file with candidates (without generating any grid). --help This help By default code is searching maximum of the F-statistic on the optimal, 4-dimensional grid (parameters of the grid, like e.g. minimal match, ale defined inside the code, in followup.c ). Main idea was to search on a denser grid than in search code, but to focus only on a few points around candidate. Additionally, when the point (on the optimal grid) with the maximal value of the F-statistic is established, one can run one of the direct maximum search algorithm: Nelder-Mead ( --simplex switch) or MADS ( --mads switch), to determine parameters of the maximum more precisely. In the current version of the code, by using --mads switch, one will use modified, 'inverted' MADS. To skip calculations on the optimal grid and run one of the algorithms directly from the initial point, use --onepoint switch.","title":"Switches"},{"location":"followup/#input-data","text":"Files required to run the code: xdatc* , DetSSB.bin and grid.bin . Path to the directory is taken from -data switch; there directories named as iii/H1 , iii/L1 etc. are expected, where iii is a frame number taken from -ident switch. Initial point of the calculations (switch -candidates ) is required. Usually the best result from coincidences (previous step of the pipeline) is taken. Parameters of the candidate need to be written into the file, as: frequency spin-down declination right_ascension","title":"Input data"},{"location":"followup/#minimal-example-how-to-run-a-code","text":"Run: LD_LIBRARY_PATH=../../search/network/src-cpu/lib/yeppp-1.0.0/binaries/linux/x86_64 ./followup -data data_path/ -band 0666 -dt 16 -candidates path_to_candidate/candidate.txt -ident 001 -nod 6 --mads> output.txt As a result (last line in the output file) one can get parameters of the found maximum: frequency spin-down declination right_ascension F-statistic_value SNR","title":"Minimal example how to run a code"},{"location":"grid_generation/","text":"Grid generation Implementation of optimal grid of templates The implementation of the banks of templates for all-sky narrow-band searches of gravitational waves from spinning neutron stars paper can be found here . Prerequisites C++ compiler. Example Using the test Gaussian data : % ./gridgen -m 0.5 -p dfg -d ../testdata/2d_0.25/001/H1/ -n 17 where -m is the minimal-match parameter -d is the directory with the ephemerids data -n is the \\log_2 \\log_2 of the number of data points -p denotes the full output (density of covering, Fisher and grid matrices) The output is grid.bin will be saved in ../testdata/2d_0.25/001/H1/grid.bin Covariance, Density of covering Density of covering: 0.25 1.76768559133 fftpad: 1 Normalized grid matrix: 4.793689962143e-05 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 -1.742438091399e-04 2.161320668089e-09 0.000000000000e+00 0.000000000000e+00 -2.479053689156e-02 -1.684378936606e-09 2.558634258126e+02 0.000000000000e+00 9.606645586405e-03 3.102095432504e-09 -1.321384079961e+02 1.941709922254e+02 Fisher matrix: 8.333333333333e-02 8.333333333333e-02 8.125315899793e-06 1.296986289673e-06 8.333333333333e-02 8.888888888889e-02 8.127119310149e-06 1.288789877048e-06 8.125315899793e-06 8.127119310149e-06 7.922523031349e-10 1.264584758632e-10 1.296986289673e-06 1.288789877048e-06 1.264584758632e-10 2.020158195389e-11 Grid matrix: 4.130435018981e+00 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 -1.501354357073e+01 1.604615232547e+01 0.000000000000e+00 0.000000000000e+00 -2.136051820725e+03 -1.250522487924e+01 2.204621622171e+07 0.000000000000e+00 8.277470103070e+02 2.303068516072e+01 -1.138557378658e+07 1.673054937411e+07 Full description of options DESCRIPTION GridsGenerator (GG) is designated to be used in all-sky narrow-band searches of continuous gravitational waves. Program allow to: - generate efficient grid(s) for chosen initial time of observation (1). - generate reduced Fisher matrix for chosen initial time of observation (1), - generate density of covering (2). (1) To get result, ephemeris must be provided. (2) Result can be get without ephemeris (for more information see flags: -nd (--ndata)). FLAGS Flags with (optional) argument(s): -c or --covariance <min> <max> <step> Covariance. Flag -c is required (even without argument) to get result. <min> - minimum value of covariance but not less than 0; default set: 0.75. <max> - optional maximum value of covariance but less than 1. <step> - optional step value of covariance; default set: 0.01. # Calculation are preform only in two cases: # 1. No flag are provided. Sets are read from file 'gg.ini'. # Result(s) is(are) printed to file(s) - see configuration file: 'gg.ini'. # 2. Flag -c (--covariance) or -m (--match) is used. # Result(s) is(are) printed to tty. -m or --match <min> <max> <step> Minimal match (MM^2 == covariance). <min> - minimum value of minimal match but not less than 0; default set: MM^2 = 0.75. <max> - optional maximum value of minimal match but less than 1. <step> - optional step value of minimal match; default set: 0.1. ## If flags -c (--covariance) and -m (--match) provided simultaneously, ## program will read options from -c flag only. -d or --directory <path> Directory containing ephemeris (need to contain binary files: 'rSSB.bin', 'rDet.bin', 'rDetSSB.bin'). <path> - take path to directory. E.g. -d 001: directory '001' located inside folder with GridsGenerator. If directory is not provided, program will try to find ephemeris in directory with GridsGenerator. -i or --initial <min> <max> <step> Initial time of observation. <min> - minimum value of initial time; default set: 0.5. <max> - optional maximum value of initial time. <step> - optional step value of minimal match; if not provided step will be set on step = max - min. -a or --algorithm <type> Algorithm type to choose. <type> take option: s1, s2, a. Algorithms: s1 - based on fully analytic formula, s2 - partially numeric formula. Accuracy for algorithm 's2' depended on -na (--nalpha) and -nr (--nradius) flags. a - automatically choose algorithm 's1' or 's2'. Use this argument to allow GridsGenerator to decide which algorithm (for given parameters) should be used to get grid with better density of covering. Information about implemented algorithms can be found in article: http://dx.doi.org/10.1088/0264-9381/32/14/145014 Default set: a. -n or --nfft <int> Number of Fourier bins. <int> take positive integer number without zero (exponent). E.g. to get grid destined to work with discreet Fourier transform (DFT, FFT) with length 1024 put same exponent of 2: 10 (1024 == 2^10). Default set: 20 (1048576 = 2^20). -nd or --ndata <int> Number of data (data length collected by detector, equal to length of ephemeris). <int> take positive integer number including zero. If <int> is set to zero data length will be read from ephemeris*. If <int> is bigger than zero program will able to obtain density of covering only**. Default set: 0. * With this set (-nd 0 or -ndata 0) density is obtained without using information stored in file 'dataS2.txt'. ### With this set density of covering for algorithm 's2' is always ### obtaining with maximal accuracy, depending only from flags: -na, -nr. ### Density of covering for algorithm 's1' is always obtained with maximal ### accuracy regardless to sets of -nd flag. ** Ephemeris are not used and because of that grid(s) and reduced Fisher matrix cannot be obtained. Density of covering for algorithm 's2' will be be obtained in approximated way based on information*** collected (in 'dataS2.txt' file) during previous runs with ephemeris. File 'dataS2.txt' collect information about coverings only for algorithm 's2'. Program inform which algorithm has been used only when work without ephemeris (<int> bigger that zero). E.g. (without ephemeris) -nd 344656: Covariance, Density, Algorithm 0.86 1.82 s2 0.87 1.8494 s1 0.88 1.77685 s1 E.g. (data length taken from ephemeris) -nd 0: Covariance, Density of covering 0.86 1.82299890516 0.87 1.84940429814 0.88 1.77685017541 *** Information stored in file 'dataS2.txt' are independent of ephemeris. Based on this information density of covering can be obtained fast but in approximated way (generally speaking more collected data allows to get more accurate results). -na or --nalpha <int> Number of loops (incrementation deep) in root finding algorithm. <int> take positive integer number without zero. This flag affect only on 's2' algorithm. Default set: 35. -nr or --nradius <int> Number of loops in covering radius (deep hole) finding algorithm. <int> take positive integer number without zero. This flag affect only on 's2' algorithm. Default set: 20. -cv or --convert <bool> Convert grid from space with hyper-sphere to hyper-ellipsoid space. <bool> take argument: t (true), f (false). Default set: t. -ch or --chop <bool> Chop result to zero if value of result is smaller than |10^-12|. <bool> take argument: t (true), f (false). Default set: f. -p or --print <string> Print result(s). <string> take argument(s): d, f, g. d - density of covering, f - Fisher reduced matrix, g - grid. Option can be joined (order is not important), e.g. df, dg, fg, dfg. Default set: g. #### If argument of flag -nd is set to be bigger than zero, flag -p #### can accept only 'd' argument. Flags with no arguments: -h or --help Display this help. -ht or --helptxt Print this help to text file. -v or --version Display information about version of GridsGenerator. -aa or --author About author(s). Display information about developer(s).","title":"Grid generation"},{"location":"grid_generation/#grid-generation","text":"","title":"Grid generation"},{"location":"grid_generation/#implementation-of-optimal-grid-of-templates","text":"The implementation of the banks of templates for all-sky narrow-band searches of gravitational waves from spinning neutron stars paper can be found here .","title":"Implementation of optimal grid of templates"},{"location":"grid_generation/#prerequisites","text":"C++ compiler.","title":"Prerequisites"},{"location":"grid_generation/#example","text":"Using the test Gaussian data : % ./gridgen -m 0.5 -p dfg -d ../testdata/2d_0.25/001/H1/ -n 17 where -m is the minimal-match parameter -d is the directory with the ephemerids data -n is the \\log_2 \\log_2 of the number of data points -p denotes the full output (density of covering, Fisher and grid matrices) The output is grid.bin will be saved in ../testdata/2d_0.25/001/H1/grid.bin Covariance, Density of covering Density of covering: 0.25 1.76768559133 fftpad: 1 Normalized grid matrix: 4.793689962143e-05 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 -1.742438091399e-04 2.161320668089e-09 0.000000000000e+00 0.000000000000e+00 -2.479053689156e-02 -1.684378936606e-09 2.558634258126e+02 0.000000000000e+00 9.606645586405e-03 3.102095432504e-09 -1.321384079961e+02 1.941709922254e+02 Fisher matrix: 8.333333333333e-02 8.333333333333e-02 8.125315899793e-06 1.296986289673e-06 8.333333333333e-02 8.888888888889e-02 8.127119310149e-06 1.288789877048e-06 8.125315899793e-06 8.127119310149e-06 7.922523031349e-10 1.264584758632e-10 1.296986289673e-06 1.288789877048e-06 1.264584758632e-10 2.020158195389e-11 Grid matrix: 4.130435018981e+00 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 -1.501354357073e+01 1.604615232547e+01 0.000000000000e+00 0.000000000000e+00 -2.136051820725e+03 -1.250522487924e+01 2.204621622171e+07 0.000000000000e+00 8.277470103070e+02 2.303068516072e+01 -1.138557378658e+07 1.673054937411e+07","title":"Example"},{"location":"grid_generation/#full-description-of-options","text":"DESCRIPTION GridsGenerator (GG) is designated to be used in all-sky narrow-band searches of continuous gravitational waves. Program allow to: - generate efficient grid(s) for chosen initial time of observation (1). - generate reduced Fisher matrix for chosen initial time of observation (1), - generate density of covering (2). (1) To get result, ephemeris must be provided. (2) Result can be get without ephemeris (for more information see flags: -nd (--ndata)). FLAGS Flags with (optional) argument(s): -c or --covariance <min> <max> <step> Covariance. Flag -c is required (even without argument) to get result. <min> - minimum value of covariance but not less than 0; default set: 0.75. <max> - optional maximum value of covariance but less than 1. <step> - optional step value of covariance; default set: 0.01. # Calculation are preform only in two cases: # 1. No flag are provided. Sets are read from file 'gg.ini'. # Result(s) is(are) printed to file(s) - see configuration file: 'gg.ini'. # 2. Flag -c (--covariance) or -m (--match) is used. # Result(s) is(are) printed to tty. -m or --match <min> <max> <step> Minimal match (MM^2 == covariance). <min> - minimum value of minimal match but not less than 0; default set: MM^2 = 0.75. <max> - optional maximum value of minimal match but less than 1. <step> - optional step value of minimal match; default set: 0.1. ## If flags -c (--covariance) and -m (--match) provided simultaneously, ## program will read options from -c flag only. -d or --directory <path> Directory containing ephemeris (need to contain binary files: 'rSSB.bin', 'rDet.bin', 'rDetSSB.bin'). <path> - take path to directory. E.g. -d 001: directory '001' located inside folder with GridsGenerator. If directory is not provided, program will try to find ephemeris in directory with GridsGenerator. -i or --initial <min> <max> <step> Initial time of observation. <min> - minimum value of initial time; default set: 0.5. <max> - optional maximum value of initial time. <step> - optional step value of minimal match; if not provided step will be set on step = max - min. -a or --algorithm <type> Algorithm type to choose. <type> take option: s1, s2, a. Algorithms: s1 - based on fully analytic formula, s2 - partially numeric formula. Accuracy for algorithm 's2' depended on -na (--nalpha) and -nr (--nradius) flags. a - automatically choose algorithm 's1' or 's2'. Use this argument to allow GridsGenerator to decide which algorithm (for given parameters) should be used to get grid with better density of covering. Information about implemented algorithms can be found in article: http://dx.doi.org/10.1088/0264-9381/32/14/145014 Default set: a. -n or --nfft <int> Number of Fourier bins. <int> take positive integer number without zero (exponent). E.g. to get grid destined to work with discreet Fourier transform (DFT, FFT) with length 1024 put same exponent of 2: 10 (1024 == 2^10). Default set: 20 (1048576 = 2^20). -nd or --ndata <int> Number of data (data length collected by detector, equal to length of ephemeris). <int> take positive integer number including zero. If <int> is set to zero data length will be read from ephemeris*. If <int> is bigger than zero program will able to obtain density of covering only**. Default set: 0. * With this set (-nd 0 or -ndata 0) density is obtained without using information stored in file 'dataS2.txt'. ### With this set density of covering for algorithm 's2' is always ### obtaining with maximal accuracy, depending only from flags: -na, -nr. ### Density of covering for algorithm 's1' is always obtained with maximal ### accuracy regardless to sets of -nd flag. ** Ephemeris are not used and because of that grid(s) and reduced Fisher matrix cannot be obtained. Density of covering for algorithm 's2' will be be obtained in approximated way based on information*** collected (in 'dataS2.txt' file) during previous runs with ephemeris. File 'dataS2.txt' collect information about coverings only for algorithm 's2'. Program inform which algorithm has been used only when work without ephemeris (<int> bigger that zero). E.g. (without ephemeris) -nd 344656: Covariance, Density, Algorithm 0.86 1.82 s2 0.87 1.8494 s1 0.88 1.77685 s1 E.g. (data length taken from ephemeris) -nd 0: Covariance, Density of covering 0.86 1.82299890516 0.87 1.84940429814 0.88 1.77685017541 *** Information stored in file 'dataS2.txt' are independent of ephemeris. Based on this information density of covering can be obtained fast but in approximated way (generally speaking more collected data allows to get more accurate results). -na or --nalpha <int> Number of loops (incrementation deep) in root finding algorithm. <int> take positive integer number without zero. This flag affect only on 's2' algorithm. Default set: 35. -nr or --nradius <int> Number of loops in covering radius (deep hole) finding algorithm. <int> take positive integer number without zero. This flag affect only on 's2' algorithm. Default set: 20. -cv or --convert <bool> Convert grid from space with hyper-sphere to hyper-ellipsoid space. <bool> take argument: t (true), f (false). Default set: t. -ch or --chop <bool> Chop result to zero if value of result is smaller than |10^-12|. <bool> take argument: t (true), f (false). Default set: f. -p or --print <string> Print result(s). <string> take argument(s): d, f, g. d - density of covering, f - Fisher reduced matrix, g - grid. Option can be joined (order is not important), e.g. df, dg, fg, dfg. Default set: g. #### If argument of flag -nd is set to be bigger than zero, flag -p #### can accept only 'd' argument. Flags with no arguments: -h or --help Display this help. -ht or --helptxt Print this help to text file. -v or --version Display information about version of GridsGenerator. -aa or --author About author(s). Display information about developer(s).","title":"Full description of options"},{"location":"input_data/","text":"Input data generation General scheme The input data for TDFstat pipeline are generated from SFDB files created by the Rome Group using PSS library (login required). Those files contain wide-band (e.g. 2048 Hz) Short-time Fourier Transforms. Two codes are used to create long, time domain series suitable for TDFstat pipeline: extract_band_hdf_td - extracts narrow band Short-Time Fourier Transforms from the SFDB, performs inverse FFT and writes Short Time Series (STS) into a HDF5 file. This code is based on the original extract_band code from PSS which only extracts narrow band SFTs and writes them to text files. genseg - combines STS-es into longer time segments, applies regions mask (e.g. regions with Analysis Ready flag) and cleans the data (outliers removal). In addition the code generates ephemeris files for coresponding time segments. The general scheme of data generation is illustrated in the picture. Example Typical All-sky search data generation workflow with O3 numbers: We used SFDB files with 2048 Hz bandwidth which contain STFT of length 1024 s overlapped in time by half (512 s). extract_band was used to extract narrow bands with B=0.25\\ Hz B=0.25\\ Hz . The resulting sampling interval was dt = 1/(2\\ B) = 2\\ s dt = 1/(2\\ B) = 2\\ s . Since STFT length is 1024 s we have 512 samples/STFT, but we use only middle half of the inverse FFT which makes 256 samples per time chunk. genseg was used to assemble short time chunks into segments of length 6 sidereal days which contain N samples: N = round(nod\\cdot C\\_SIDDAY/dt) = 258492 N = round(nod\\cdot C\\_SIDDAY/dt) = 258492 where C\\_SIDDAY C\\_SIDDAY is the duration of sidereal day in seconds. The whole O3 run time span was ~1 yr so we've got 60 segments. Band and segment numbering For practical reasons we assingn integer numbers to our frequency bands according to this general formula: fpo = 10 + (1 - 2^{-ov})\\cdot bbbb\\cdot B \\quad \\mathrm{[Hz]}. fpo = 10 + (1 - 2^{-ov})\\cdot bbbb\\cdot B \\quad \\mathrm{[Hz]}. where: fpo fpo - starting/reference frequency of the band, bbbb bbbb - integer band number, B B - bandwidth, 2^{-ov} 2^{-ov} - band overlap; oi oi is natural number; this form assures alignement of bands with Fourier bins in the SFDB. In the names of files and directories, the band number , bbbb bbbb is formatted using %04d specifier, e.g. 0072 . The time segment (or simply 'segment') has length nod nod which is INTEGER number of days. Segments are assigned subsequent natural numbers starting from 1. In the names of files and directories the segment number ( nnn nnn ) is formatted using %03d specifier, e.g. 001 . extract_band code This program converts Short Fourier Transformation series into time series. It was written by Pia Astone (INFN, Physics Department of University of Rome \"La Sapienza\") a part of the PSS package used to create SFDB. The extract_band_hdf_td code is its substantially modified version by Pawe\u0142 Cieciel\u0105g, used in TDFstat since run O4. The major differences comprise: inverse FFT to get short time series (STS) and output to HDF5 file. Compilation Source: currently part of a private gitlab repo Prerequisities: C compiler, selected files from the PSS library ( PSS library ), FFTW3 & HDF5 libraries. To compile, in the extract_band/pss_sfdb directory type make extract_band_hdf_td Running extract_band_hdf_td reads following input parameters from stdin: string[170] out_file: output file name string[170] in_file: input file name (list of all SFDB files) float fpo: reference frequency (starting frequency of the band) float B: bandwidth e.g. create file 0072_H1.in sfft_0072_H1.h5 ../../sfdb_files_H1.list 26.2 0.25 and pass it as input: ./extract_band_hdf_td < 0072_H1.in Remarks: The file names can be specified with absolute or relative paths. The code is designed to work in an incremental mode - if you add more files to in_file and out_file exists, the new data chunks will be appended to it. For chunks already present in the HDF file only attributes mjdtime, nfft and sfdb_name are verified, no actual data is read. If the out_file is corrupted (broken HDF5 structure) it will be re-created from the beginning. the code is very generic - it does not know which band number is processed (but it can be encoded in the name of the input file). Helper script To simplify mass data generation, e.g. for all-sky search, we provide an example of bash script: eb2hdf.sh . It uses extract_band is to create STS data for all bands and all detectors following certain convention. Before using the script, first edit some parameters in section marked \"EDIT HERE\": eb - full path to the extract_band_hdf_td executable B and ov - bandwidth and overlap as defined above ilist - path to file containing list of SFDB files to be processed; default: ./sfdb_${det}.list odir - path to the output directory (will be created); default: sts_B${B}_ov${ov}/${det} fpo - check the definition of fpo( <band_number> ) function To generate HDF file for detector <det_name> and for bands between <start_band> and <end_band> as follows run: ./eb2hdf.sh <det_name> <start_band> [<end_band>] Currently det_name can be H1, L1 or V1. If <end_band> is omitted then only single band is generated. The ilist file should contain one file per line, to create use: ls -1 /somedirectory/*.SFDB09 The input file for extract_band will be saved in: sts_B${B}_ov${ov}/${det}/${B4}_${det}.in The standard output from extract_band will be saved in: sts_B${B}_ov${ov}/${det}/eb2hdf-${B4}_${det}.out Output HDF5 file format To display contents of the HDF5 file use: h5dump -q creation_order <file_name> Our HDF files have creation_order tracking and indexing flags enabled for easier sorting of chucks (typically the chunks will be created in an increasing ichunk order). The internal file structure: HDF5 object | data type ---------------------------------------------------- / | root group \u251c\u2500\u2500 attr: format_version | int \u251c\u2500\u2500 attr: site | str[3] \u251c\u2500\u2500 attr: fpo | double \u251c\u2500\u2500 attr: bandwidth | float \u251c\u2500\u2500 attr: df | double \u251c\u2500\u2500 attr: dtype | str[4] \u251c\u2500\u2500 attr: sft_overlap | int \u251c\u2500\u2500 attr: nsamples | int \u251c\u2500\u2500 attr: scaling_factor | double \u251c\u2500\u2500 attr: subsampling_factor | double \u251c\u2500\u2500 attr: last_ichunk | int \u251c\u2500\u2500 dataset \"ichunk\" | dataset \u2502 \u251c\u2500\u2500 data | float data[nsamples] \u2502 \u251c\u2500\u2500 attr: ichunk | int \u2502 \u251c\u2500\u2500 attr: gps_sec | int \u2502 \u251c\u2500\u2500 attr: gps_nsec | int \u2502 \u251c\u2500\u2500 attr: sft_mjdtime | double \u2502 \u251c\u2500\u2500 attr: sfdb_file | str[] \u2502 \u251c\u2500\u2500 attr: ctime | str[] \u2502 \u2514\u2500\u2500 attr: nfft | int \u2514\u2500\u2500 dataset \"ichunk+1\" \u251c\u2500\u2500 data \u2514\u2500\u2500 ... Please refer to the genseg code for example how to read this HDF5 file in C. Old/original extract_band Click to expand Running: % extract_band < input_file where `input_file` is an ASCII file containing the following rows: * Maximal number of SFT * The name of the output file * The list of SFT files * The frequency band in Hz * The width of frequency band in Hz e.g., 100000 J0034+1612_2010-10-10.out J0034+1612_2010-10-10.list 718.2480 1 Output to a text file: % Beginning freq- Band- Samples in one stretch- Subsampling factor- inter (overlapping, 2 if data were overlapped)- Frequency step- Scaling factor- ***The data are real and imag of the FFT % 908.152344 0.250000 256 8192.000000 2 0.0009766 1.000000e-20 % FFT number in the file; Beginning mjd days; Gps s; Gps ns; % 100 55099.5879745370 937922816 0 4.59662571e+02 2.27630825e+01 -3.50387007e+02 -2.20005558e+02 3.57587904e+02 1.01217077e+02 1.74400486e+02 2.62086552e+02 2.21804800e+02 -5.20278366e+02 -3.87826732e+02 -1.55758978e+02 genseg code This code combines STS (short time series) chunks stored in a HDF5 file (produced by extract_band ) into time segments of any length. In addition: it applies science (analysis ready) mask (Tukey window is used to smooth edges of each continuous region) removes outliers writes ephemeris files (DetSSB.bin) for the detector (requires lalsuite) Compilation: Source code Prerequisities: C compiler, HDF5 library, lalsuite (if USE_LAL=yes is set in the Makefile - this is required to generate ephemeris) lalsuite can be installed from conda-forge repository using miniforge installer : mamba create -n lal lalsuite mamba activate lal To compile type make genseg-hdf . If USE_LAL=yes then the executable will use runtime library path $CONDA_PREFIX/lib (to search for lal libraries). The old genseg version (used before O4) is preserved in subdirectory old . Running: genseg-hdf requires a configuration file in the INI format. An example config file is provided in the source directory H1_0072_6d.g2d . All configuration parameters are explained in the comments in this file. To generate list of Analysis Ready segments this python script can be used: from gwpy.segments import DataQualityFlag, SegmentList import sys GPSstart = int(sys.argv[1]) # 1238166018 (O3 start - 2019-04-01T15:00:00) GPSend = int(sys.argv[2]) # 1269363618 (O3 end - 2020-03-27T17:00:00 ) det_channel_name = sys.argv[3] # e.g. 'H1:DMT-ANALYSIS_READY:1' or 'H1:DCS-ANALYSIS_READY_C01:1' dqf = DataQualityFlag.query(det_channel_name, GPSstart, GPSend) seg_science = SegmentList(dqf.active) for s in seg_science: print(s.start, s.end) python get_sci_segment_list.py 1238166018 1269363618 H1:DMT-ANALYSIS_READY:1 > O3_C00_H1_sci_segments python get_sci_segment_list.py 1238166018 1269363618 L1:DMT-ANALYSIS_READY:1 > O3_C00_L1_sci_segments python get_sci_segment_list.py 1238166018 1269363618 V1:ITF_SCIENCE:1 > O3_C00_V1_sci_segments To calculate ephemeris the code should be run under proper conda environment and two efemerid files must be specified in the config file. They can be extracted from lalpulsar to te current directory this way: cp $CONDA_PREFIX/share/lalpulsar/sun00-40-DE405.dat.gz . cp $CONDA_PREFIX/share/lalpulsar/earth00-40-DE405.dat.gz . gunzip sun00-40-DE405.dat.gz earth00-40-DE405.dat.gz To generate time segments run: ./genseg-hdf <config_file> [bbbb] The last parameter, band number written as 4 character, zero padded string bbbb , is optional. It is introduced to avoid creation of tousands of config files (e.g. all-sky case) which differ only by the band number. In such case, one can use single config file but two parameters, infile and plsr have to contain string bbbb e.g.: infile = sts_B0.25_ov0.1/H1/sts_bbbb_H1.h5 plsr = bbbb Then the command line string will be inserted in place of bbbb string in those parameters, e.g.: ./genseg-hdf H1_bbbb_6d.g2d 0072 will generate segments for band 72. TDFstat input data structure The TDFstat pipeline (actually the mein search code) assumes that input data have fixed directory and fileneme structure. The input data are divided into time segments of typically a few days length and consists - for each detector - of the input time series data, the ephemerides and the grid-generating matrix file (defining the parameter space of the search). A single search run requires 2 data files for each detector DD and segment nnn , stored in data_dir/nnn/DD subdirectory, where DD is currently either H1 (Hanford), L1 (Livingston) or V1 (Virgo Cascina): xdat_nnn_bbbb.bin - time-domain narrow-band data sequence ( bbbb is the number of frequency band), DetSSB.bin - location of the detector w.r.t. the Solar System Barycenter (SSB), in Cartesian coordinates, sampled at dt sampling rate (array of size 2N ). The last two records in this file are the angle phir , determining the position of Earth in its diurnal motion, and the obliquity of the ecliptic epsm , both calculated for the first sample of the data. Third file is the sky positions-frequency-spindown grid file in linear coordinates (common for all the detectors), stored in data_dir/nnn in case of the network search (one grid file is used by all the detectors) or in each detector directory separately (in case of single-detector searches): grid.bin - generator matrix of an optimal grid of templates (defining the parameter space; see here for details). A typical directory structure is as follows: xdat_O3_C01/ \u2514\u2500\u2500 001/ \u251c\u2500\u2500 grids \u2502 \u2514\u2500\u2500 grid_001_1234_H1L1.bin \u251c\u2500\u2500 H1/ \u2502 \u251c\u2500\u2500 DetSSB.bin \u2502 \u251c\u2500\u2500 grid.bin \u2502 \u251c\u2500\u2500 starting_date \u2502 \u2514\u2500\u2500 xdat_001_1234.bin \u2514\u2500\u2500 L1/ \u251c\u2500\u2500 DetSSB.bin \u251c\u2500\u2500 grid.bin \u251c\u2500\u2500 starting_date \u2514\u2500\u2500 xdat_001_1234.bin Beginning of each time frame is saved in the nnn/DD/starting_date file, e.g., % cat 2d_0.25/001/H1/starting_date 1.1260846080e+09 With this structure the root directory xdat_O3_C01 is passed to the search code as parameter indir . An example for two LIGO detectors H1 and L1, and data frame segments nnn=001-008 nnn=001-008 with pure Gaussian noise 2-day time segments with sampling time equal to 2s for a fiducial narrow band number bbbb=1234 bbbb=1234 ( xdatc_nnn_1234.bin ) coresponding the the band frequency fpo=308.859375 fpo=308.859375 is available here . Gaussian data generator genseg directory contains additional code gauss-xdat-mask.c to generate time series drawn from the Gaussian distribution. Compilation Prerequisites: C compiler , GSL library % gcc gauss--xdat-mask.c -o gauss-xdat-mask -lm -lgsl -lgslcblas Running The program takes input values from the command line: % ./gauss-xdat-mask N amplitude sigma output-file [xdat-template] where N is the length of the time series (in samples), amplitude and sigma are Gauss function parameters, output-file is the name of the output file and xdat-template is an optional parameter: name of the existing xdat file from which only gaps (zeros) will be copied to the output (e.g. the real xdat file - usefull for testing). Example: % ./gauss-xdat-mask 86164 1 1 xdat_001_1234.bin The output is a binary file, xdat_001_1234.bin containing 86164 float-precision numbers.","title":"Input data generation"},{"location":"input_data/#input-data-generation","text":"","title":"Input data generation"},{"location":"input_data/#general-scheme","text":"The input data for TDFstat pipeline are generated from SFDB files created by the Rome Group using PSS library (login required). Those files contain wide-band (e.g. 2048 Hz) Short-time Fourier Transforms. Two codes are used to create long, time domain series suitable for TDFstat pipeline: extract_band_hdf_td - extracts narrow band Short-Time Fourier Transforms from the SFDB, performs inverse FFT and writes Short Time Series (STS) into a HDF5 file. This code is based on the original extract_band code from PSS which only extracts narrow band SFTs and writes them to text files. genseg - combines STS-es into longer time segments, applies regions mask (e.g. regions with Analysis Ready flag) and cleans the data (outliers removal). In addition the code generates ephemeris files for coresponding time segments. The general scheme of data generation is illustrated in the picture.","title":"General scheme"},{"location":"input_data/#example","text":"Typical All-sky search data generation workflow with O3 numbers: We used SFDB files with 2048 Hz bandwidth which contain STFT of length 1024 s overlapped in time by half (512 s). extract_band was used to extract narrow bands with B=0.25\\ Hz B=0.25\\ Hz . The resulting sampling interval was dt = 1/(2\\ B) = 2\\ s dt = 1/(2\\ B) = 2\\ s . Since STFT length is 1024 s we have 512 samples/STFT, but we use only middle half of the inverse FFT which makes 256 samples per time chunk. genseg was used to assemble short time chunks into segments of length 6 sidereal days which contain N samples: N = round(nod\\cdot C\\_SIDDAY/dt) = 258492 N = round(nod\\cdot C\\_SIDDAY/dt) = 258492 where C\\_SIDDAY C\\_SIDDAY is the duration of sidereal day in seconds. The whole O3 run time span was ~1 yr so we've got 60 segments.","title":"Example"},{"location":"input_data/#band-and-segment-numbering","text":"For practical reasons we assingn integer numbers to our frequency bands according to this general formula: fpo = 10 + (1 - 2^{-ov})\\cdot bbbb\\cdot B \\quad \\mathrm{[Hz]}. fpo = 10 + (1 - 2^{-ov})\\cdot bbbb\\cdot B \\quad \\mathrm{[Hz]}. where: fpo fpo - starting/reference frequency of the band, bbbb bbbb - integer band number, B B - bandwidth, 2^{-ov} 2^{-ov} - band overlap; oi oi is natural number; this form assures alignement of bands with Fourier bins in the SFDB. In the names of files and directories, the band number , bbbb bbbb is formatted using %04d specifier, e.g. 0072 . The time segment (or simply 'segment') has length nod nod which is INTEGER number of days. Segments are assigned subsequent natural numbers starting from 1. In the names of files and directories the segment number ( nnn nnn ) is formatted using %03d specifier, e.g. 001 .","title":"Band and segment numbering"},{"location":"input_data/#extract_band-code","text":"This program converts Short Fourier Transformation series into time series. It was written by Pia Astone (INFN, Physics Department of University of Rome \"La Sapienza\") a part of the PSS package used to create SFDB. The extract_band_hdf_td code is its substantially modified version by Pawe\u0142 Cieciel\u0105g, used in TDFstat since run O4. The major differences comprise: inverse FFT to get short time series (STS) and output to HDF5 file.","title":"extract_band code"},{"location":"input_data/#compilation","text":"Source: currently part of a private gitlab repo Prerequisities: C compiler, selected files from the PSS library ( PSS library ), FFTW3 & HDF5 libraries. To compile, in the extract_band/pss_sfdb directory type make extract_band_hdf_td","title":"Compilation"},{"location":"input_data/#running","text":"extract_band_hdf_td reads following input parameters from stdin: string[170] out_file: output file name string[170] in_file: input file name (list of all SFDB files) float fpo: reference frequency (starting frequency of the band) float B: bandwidth e.g. create file 0072_H1.in sfft_0072_H1.h5 ../../sfdb_files_H1.list 26.2 0.25 and pass it as input: ./extract_band_hdf_td < 0072_H1.in Remarks: The file names can be specified with absolute or relative paths. The code is designed to work in an incremental mode - if you add more files to in_file and out_file exists, the new data chunks will be appended to it. For chunks already present in the HDF file only attributes mjdtime, nfft and sfdb_name are verified, no actual data is read. If the out_file is corrupted (broken HDF5 structure) it will be re-created from the beginning. the code is very generic - it does not know which band number is processed (but it can be encoded in the name of the input file).","title":"Running"},{"location":"input_data/#helper-script","text":"To simplify mass data generation, e.g. for all-sky search, we provide an example of bash script: eb2hdf.sh . It uses extract_band is to create STS data for all bands and all detectors following certain convention. Before using the script, first edit some parameters in section marked \"EDIT HERE\": eb - full path to the extract_band_hdf_td executable B and ov - bandwidth and overlap as defined above ilist - path to file containing list of SFDB files to be processed; default: ./sfdb_${det}.list odir - path to the output directory (will be created); default: sts_B${B}_ov${ov}/${det} fpo - check the definition of fpo( <band_number> ) function To generate HDF file for detector <det_name> and for bands between <start_band> and <end_band> as follows run: ./eb2hdf.sh <det_name> <start_band> [<end_band>] Currently det_name can be H1, L1 or V1. If <end_band> is omitted then only single band is generated. The ilist file should contain one file per line, to create use: ls -1 /somedirectory/*.SFDB09 The input file for extract_band will be saved in: sts_B${B}_ov${ov}/${det}/${B4}_${det}.in The standard output from extract_band will be saved in: sts_B${B}_ov${ov}/${det}/eb2hdf-${B4}_${det}.out","title":"Helper script"},{"location":"input_data/#output-hdf5-file-format","text":"To display contents of the HDF5 file use: h5dump -q creation_order <file_name> Our HDF files have creation_order tracking and indexing flags enabled for easier sorting of chucks (typically the chunks will be created in an increasing ichunk order). The internal file structure: HDF5 object | data type ---------------------------------------------------- / | root group \u251c\u2500\u2500 attr: format_version | int \u251c\u2500\u2500 attr: site | str[3] \u251c\u2500\u2500 attr: fpo | double \u251c\u2500\u2500 attr: bandwidth | float \u251c\u2500\u2500 attr: df | double \u251c\u2500\u2500 attr: dtype | str[4] \u251c\u2500\u2500 attr: sft_overlap | int \u251c\u2500\u2500 attr: nsamples | int \u251c\u2500\u2500 attr: scaling_factor | double \u251c\u2500\u2500 attr: subsampling_factor | double \u251c\u2500\u2500 attr: last_ichunk | int \u251c\u2500\u2500 dataset \"ichunk\" | dataset \u2502 \u251c\u2500\u2500 data | float data[nsamples] \u2502 \u251c\u2500\u2500 attr: ichunk | int \u2502 \u251c\u2500\u2500 attr: gps_sec | int \u2502 \u251c\u2500\u2500 attr: gps_nsec | int \u2502 \u251c\u2500\u2500 attr: sft_mjdtime | double \u2502 \u251c\u2500\u2500 attr: sfdb_file | str[] \u2502 \u251c\u2500\u2500 attr: ctime | str[] \u2502 \u2514\u2500\u2500 attr: nfft | int \u2514\u2500\u2500 dataset \"ichunk+1\" \u251c\u2500\u2500 data \u2514\u2500\u2500 ... Please refer to the genseg code for example how to read this HDF5 file in C.","title":"Output HDF5 file format"},{"location":"input_data/#oldoriginal-extract_band","text":"Click to expand Running: % extract_band < input_file where `input_file` is an ASCII file containing the following rows: * Maximal number of SFT * The name of the output file * The list of SFT files * The frequency band in Hz * The width of frequency band in Hz e.g., 100000 J0034+1612_2010-10-10.out J0034+1612_2010-10-10.list 718.2480 1 Output to a text file: % Beginning freq- Band- Samples in one stretch- Subsampling factor- inter (overlapping, 2 if data were overlapped)- Frequency step- Scaling factor- ***The data are real and imag of the FFT % 908.152344 0.250000 256 8192.000000 2 0.0009766 1.000000e-20 % FFT number in the file; Beginning mjd days; Gps s; Gps ns; % 100 55099.5879745370 937922816 0 4.59662571e+02 2.27630825e+01 -3.50387007e+02 -2.20005558e+02 3.57587904e+02 1.01217077e+02 1.74400486e+02 2.62086552e+02 2.21804800e+02 -5.20278366e+02 -3.87826732e+02 -1.55758978e+02","title":"Old/original extract_band"},{"location":"input_data/#genseg-code","text":"This code combines STS (short time series) chunks stored in a HDF5 file (produced by extract_band ) into time segments of any length. In addition: it applies science (analysis ready) mask (Tukey window is used to smooth edges of each continuous region) removes outliers writes ephemeris files (DetSSB.bin) for the detector (requires lalsuite)","title":"genseg code"},{"location":"input_data/#compilation_1","text":"Source code Prerequisities: C compiler, HDF5 library, lalsuite (if USE_LAL=yes is set in the Makefile - this is required to generate ephemeris) lalsuite can be installed from conda-forge repository using miniforge installer : mamba create -n lal lalsuite mamba activate lal To compile type make genseg-hdf . If USE_LAL=yes then the executable will use runtime library path $CONDA_PREFIX/lib (to search for lal libraries). The old genseg version (used before O4) is preserved in subdirectory old .","title":"Compilation:"},{"location":"input_data/#running_1","text":"genseg-hdf requires a configuration file in the INI format. An example config file is provided in the source directory H1_0072_6d.g2d . All configuration parameters are explained in the comments in this file. To generate list of Analysis Ready segments this python script can be used: from gwpy.segments import DataQualityFlag, SegmentList import sys GPSstart = int(sys.argv[1]) # 1238166018 (O3 start - 2019-04-01T15:00:00) GPSend = int(sys.argv[2]) # 1269363618 (O3 end - 2020-03-27T17:00:00 ) det_channel_name = sys.argv[3] # e.g. 'H1:DMT-ANALYSIS_READY:1' or 'H1:DCS-ANALYSIS_READY_C01:1' dqf = DataQualityFlag.query(det_channel_name, GPSstart, GPSend) seg_science = SegmentList(dqf.active) for s in seg_science: print(s.start, s.end) python get_sci_segment_list.py 1238166018 1269363618 H1:DMT-ANALYSIS_READY:1 > O3_C00_H1_sci_segments python get_sci_segment_list.py 1238166018 1269363618 L1:DMT-ANALYSIS_READY:1 > O3_C00_L1_sci_segments python get_sci_segment_list.py 1238166018 1269363618 V1:ITF_SCIENCE:1 > O3_C00_V1_sci_segments To calculate ephemeris the code should be run under proper conda environment and two efemerid files must be specified in the config file. They can be extracted from lalpulsar to te current directory this way: cp $CONDA_PREFIX/share/lalpulsar/sun00-40-DE405.dat.gz . cp $CONDA_PREFIX/share/lalpulsar/earth00-40-DE405.dat.gz . gunzip sun00-40-DE405.dat.gz earth00-40-DE405.dat.gz To generate time segments run: ./genseg-hdf <config_file> [bbbb] The last parameter, band number written as 4 character, zero padded string bbbb , is optional. It is introduced to avoid creation of tousands of config files (e.g. all-sky case) which differ only by the band number. In such case, one can use single config file but two parameters, infile and plsr have to contain string bbbb e.g.: infile = sts_B0.25_ov0.1/H1/sts_bbbb_H1.h5 plsr = bbbb Then the command line string will be inserted in place of bbbb string in those parameters, e.g.: ./genseg-hdf H1_bbbb_6d.g2d 0072 will generate segments for band 72.","title":"Running:"},{"location":"input_data/#tdfstat-input-data-structure","text":"The TDFstat pipeline (actually the mein search code) assumes that input data have fixed directory and fileneme structure. The input data are divided into time segments of typically a few days length and consists - for each detector - of the input time series data, the ephemerides and the grid-generating matrix file (defining the parameter space of the search). A single search run requires 2 data files for each detector DD and segment nnn , stored in data_dir/nnn/DD subdirectory, where DD is currently either H1 (Hanford), L1 (Livingston) or V1 (Virgo Cascina): xdat_nnn_bbbb.bin - time-domain narrow-band data sequence ( bbbb is the number of frequency band), DetSSB.bin - location of the detector w.r.t. the Solar System Barycenter (SSB), in Cartesian coordinates, sampled at dt sampling rate (array of size 2N ). The last two records in this file are the angle phir , determining the position of Earth in its diurnal motion, and the obliquity of the ecliptic epsm , both calculated for the first sample of the data. Third file is the sky positions-frequency-spindown grid file in linear coordinates (common for all the detectors), stored in data_dir/nnn in case of the network search (one grid file is used by all the detectors) or in each detector directory separately (in case of single-detector searches): grid.bin - generator matrix of an optimal grid of templates (defining the parameter space; see here for details). A typical directory structure is as follows: xdat_O3_C01/ \u2514\u2500\u2500 001/ \u251c\u2500\u2500 grids \u2502 \u2514\u2500\u2500 grid_001_1234_H1L1.bin \u251c\u2500\u2500 H1/ \u2502 \u251c\u2500\u2500 DetSSB.bin \u2502 \u251c\u2500\u2500 grid.bin \u2502 \u251c\u2500\u2500 starting_date \u2502 \u2514\u2500\u2500 xdat_001_1234.bin \u2514\u2500\u2500 L1/ \u251c\u2500\u2500 DetSSB.bin \u251c\u2500\u2500 grid.bin \u251c\u2500\u2500 starting_date \u2514\u2500\u2500 xdat_001_1234.bin Beginning of each time frame is saved in the nnn/DD/starting_date file, e.g., % cat 2d_0.25/001/H1/starting_date 1.1260846080e+09 With this structure the root directory xdat_O3_C01 is passed to the search code as parameter indir . An example for two LIGO detectors H1 and L1, and data frame segments nnn=001-008 nnn=001-008 with pure Gaussian noise 2-day time segments with sampling time equal to 2s for a fiducial narrow band number bbbb=1234 bbbb=1234 ( xdatc_nnn_1234.bin ) coresponding the the band frequency fpo=308.859375 fpo=308.859375 is available here .","title":"TDFstat input data structure"},{"location":"input_data/#gaussian-data-generator","text":"genseg directory contains additional code gauss-xdat-mask.c to generate time series drawn from the Gaussian distribution.","title":"Gaussian data generator"},{"location":"input_data/#compilation_2","text":"Prerequisites: C compiler , GSL library % gcc gauss--xdat-mask.c -o gauss-xdat-mask -lm -lgsl -lgslcblas","title":"Compilation"},{"location":"input_data/#running_2","text":"The program takes input values from the command line: % ./gauss-xdat-mask N amplitude sigma output-file [xdat-template] where N is the length of the time series (in samples), amplitude and sigma are Gauss function parameters, output-file is the name of the output file and xdat-template is an optional parameter: name of the existing xdat file from which only gaps (zeros) will be copied to the output (e.g. the real xdat file - usefull for testing). Example: % ./gauss-xdat-mask 86164 1 1 xdat_001_1234.bin The output is a binary file, xdat_001_1234.bin containing 86164 float-precision numbers.","title":"Running"},{"location":"search_for_candidates/","text":"F-statistic candidate signal search Production serial code for a network of detectors is available at here . OpenMP version is at this location . To get the whole pipeline, run git clone https://github.com/mbejger/polgraw-allsky.git . Algorithm flowchart Prerequisites The code is written in standard C . GNU Scientific Library (GSL) and the FFTW library (version 3.0 or later) are needed to run the code. GNU struct dirent objects are used to read the directories. Optionally, SLEEF or YEPPP! , libraries for high-performance computing that are optimized for speed are used to evaluate the trigonometric functions in the search code. These libraries are ported with the source code and are located in src/lib . The choice which of these libraries to use has to be made at compilation time by modifying the Makefile . Compilation Run make gwsearch-cpu or make in search/network/src-cpu , resulting binary is called gwsearch-cpu (this is the default C version not-optimized with openMP ; for the openMP version see the search/network/src-openmp directory). Modify the Makefile to fit your system. By default the YEPPP! library is selected. Full list of switches For the full list of options, type % ./gwsearch-cpu --help Switch Description -data Data directory (default is . ) -output Output directory (default is ./candidates ) -ident Frame number -band Band number -label Custom label for the input and output files -range Use file with grid range or pulsar position -getrange Write grid ranges & save fft wisdom & exit (ignore -r) -cwd Change to directory <dir> -threshold Threshold for the \\mathcal{F} \\mathcal{F} -statistic (default is 20 ) -hemisphere Hemisphere (default is 0 - does both) -fpo Reference band frequency fpo value -dt Data sampling time dt (default value: 0.5 ) -usedet Use only detectors from string (default is use all available ) -addsig Add signal with parameters from <file> -narrowdown Narrow-down the frequency band (range [0, 0.5] +- around center ) Also: --whitenoise White Gaussian noise assumed --nospindown Spindowns neglected --nocheckpoint State file will not be created (no checkpointing) --help This help Example Minimal call to gwsearch-cpu is as follows (code compiled with the GNUSINCOS option): % ./gwsearch-cpu -data ../../../testdata/2d_0.25 -dt 2 -output . -ident 001 -band 1234 -nod 2 where data directory is the base directory of input data files, Sampling time dt is 2 s 2 s , output directory is a directory to write the output (e.g., in current directory, . ) ident is the number of time frame to be analyzed ( 001 001 ), nod number of days is 2 2 , band is the number of the frequency band (see the input data structure for details). Network of detectors Test data frames nnn=001-008 nnn=001-008 with pure Gaussian noise 2-day time segments with sampling time equal to 2s ( xdatc_nnn_1234.bin ) for two LIGO detectors H1 and L1 are available here . A sample call is % LD_LIBRARY_PATH=lib/yeppp-1.0.0/binaries/linux/x86_64 ./gwsearch-cpu \\ -data ../../../testdata/2d_0.25/ \\ -ident 001 \\ -band 1234 \\ -dt 2 \\ -nod 2 \\ -addsig sig1 \\ -output . \\ -threshold 14.5 \\ --nocheckpoint where the LD_LIBRARY_PATH points to the location of the YEPPP! library. The program will proceed assuming that the data directory for frame 001 is located at ../../../testdata/2d_0.25/001 and contain subdirectories with input data for H1, L1 and/or V1 detectors (all available detectors are used by default; to select specific detectors, use -usedet option), the grid of parameters files is expected to be in ../../../testdata/2d_0.25/001 , band equals to 1234 1234 , the sampling time dt equals 2 s 2 s , number of days nod in 2 2 , the -addsig option is used to add a software injection to pure noise Gaussian data. The signal's parameters are randomly generated using the sigen code (for more details see the minimal example ). the threshold for the \\mathcal{F} \\mathcal{F} -statistic is set to be 14.5 14.5 , -output is the current directory, --nocheckpoint disables the checkpointing (writing the last visited position on the grid to the state file), Output files Binary output files, containing trigger candidate events above an arbitrary threshold (option -threshold for the \\mathcal{F} \\mathcal{F} -statistic, default 20), are written to the output_dir directory. There are two output files for every input data sequence: triggers_nnn_bbbb_1.bin and triggers_nnn_bbbb_2.bin , where 1 and 2 correspond to the northern and southern ecliptic hemisphere. Each trigger (candidate) event occupies 40 consecutive bytes (5 double numbers), with the following meaning: Record no. | --------------------- | ---------------------------- 1 | frequency [radians, between 0 and \\pi \\pi ] above fpo 2 | spindown [ \\mathrm{Hz/s} \\mathrm{Hz/s} ] 3 | declination [radians, between \\pi/2 \\pi/2 and -\\pi/2 -\\pi/2 ] 4 | right ascension [radians, between 0 and 2\\pi 2\\pi ] 5 | signal-to-noise ratio For the example above, the first 10 triggers from triggers_001_1234_2.bin are 3.05617018e+00 -3.42376198e-08 -7.68007347e-02 2.59248668e+00 5.06667333e+00 1.18243015e+00 -3.20762991e-08 -7.68007347e-02 2.59248668e+00 5.05528873e+00 1.08103361e-01 -2.77536578e-08 -7.68007347e-02 2.59248668e+00 5.07085254e+00 1.90022435e+00 -2.77536578e-08 -7.68007347e-02 2.59248668e+00 5.15191593e+00 1.90000217e+00 -2.55923371e-08 -7.68007347e-02 2.59248668e+00 5.42638039e+00 2.09224664e+00 -2.34310165e-08 -7.68007347e-02 2.59248668e+00 5.20879551e+00 2.38731576e+00 -2.12696958e-08 -7.68007347e-02 2.59248668e+00 5.31983396e+00 3.00543165e+00 -1.91083751e-08 -7.68007347e-02 2.59248668e+00 5.29454616e+00 7.49333983e-01 -1.26244131e-08 -7.68007347e-02 2.59248668e+00 5.08724856e+00 2.08710778e-01 3.43510887e-10 -7.68007347e-02 2.59248668e+00 5.17537018e+00 Auxiliary output files wisdom-hostname.dat - performance-testing file created by the FFTW . The hostname variable is determined by a call to gethostname() , state_nnn_bbbb.dat - checkpointing file containing the last grid point visited. The search can be safely restarted, calculations will continue from the last grid position saved to this file. After successful termination, checkpoint file is left empty.","title":"F-statistic candidate signal search"},{"location":"search_for_candidates/#f-statistic-candidate-signal-search","text":"Production serial code for a network of detectors is available at here . OpenMP version is at this location . To get the whole pipeline, run git clone https://github.com/mbejger/polgraw-allsky.git .","title":"F-statistic candidate signal search"},{"location":"search_for_candidates/#algorithm-flowchart","text":"","title":"Algorithm flowchart"},{"location":"search_for_candidates/#prerequisites","text":"The code is written in standard C . GNU Scientific Library (GSL) and the FFTW library (version 3.0 or later) are needed to run the code. GNU struct dirent objects are used to read the directories. Optionally, SLEEF or YEPPP! , libraries for high-performance computing that are optimized for speed are used to evaluate the trigonometric functions in the search code. These libraries are ported with the source code and are located in src/lib . The choice which of these libraries to use has to be made at compilation time by modifying the Makefile .","title":"Prerequisites"},{"location":"search_for_candidates/#compilation","text":"Run make gwsearch-cpu or make in search/network/src-cpu , resulting binary is called gwsearch-cpu (this is the default C version not-optimized with openMP ; for the openMP version see the search/network/src-openmp directory). Modify the Makefile to fit your system. By default the YEPPP! library is selected.","title":"Compilation"},{"location":"search_for_candidates/#full-list-of-switches","text":"For the full list of options, type % ./gwsearch-cpu --help Switch Description -data Data directory (default is . ) -output Output directory (default is ./candidates ) -ident Frame number -band Band number -label Custom label for the input and output files -range Use file with grid range or pulsar position -getrange Write grid ranges & save fft wisdom & exit (ignore -r) -cwd Change to directory <dir> -threshold Threshold for the \\mathcal{F} \\mathcal{F} -statistic (default is 20 ) -hemisphere Hemisphere (default is 0 - does both) -fpo Reference band frequency fpo value -dt Data sampling time dt (default value: 0.5 ) -usedet Use only detectors from string (default is use all available ) -addsig Add signal with parameters from <file> -narrowdown Narrow-down the frequency band (range [0, 0.5] +- around center ) Also: --whitenoise White Gaussian noise assumed --nospindown Spindowns neglected --nocheckpoint State file will not be created (no checkpointing) --help This help","title":"Full list of switches"},{"location":"search_for_candidates/#example","text":"Minimal call to gwsearch-cpu is as follows (code compiled with the GNUSINCOS option): % ./gwsearch-cpu -data ../../../testdata/2d_0.25 -dt 2 -output . -ident 001 -band 1234 -nod 2 where data directory is the base directory of input data files, Sampling time dt is 2 s 2 s , output directory is a directory to write the output (e.g., in current directory, . ) ident is the number of time frame to be analyzed ( 001 001 ), nod number of days is 2 2 , band is the number of the frequency band (see the input data structure for details).","title":"Example"},{"location":"search_for_candidates/#network-of-detectors","text":"Test data frames nnn=001-008 nnn=001-008 with pure Gaussian noise 2-day time segments with sampling time equal to 2s ( xdatc_nnn_1234.bin ) for two LIGO detectors H1 and L1 are available here . A sample call is % LD_LIBRARY_PATH=lib/yeppp-1.0.0/binaries/linux/x86_64 ./gwsearch-cpu \\ -data ../../../testdata/2d_0.25/ \\ -ident 001 \\ -band 1234 \\ -dt 2 \\ -nod 2 \\ -addsig sig1 \\ -output . \\ -threshold 14.5 \\ --nocheckpoint where the LD_LIBRARY_PATH points to the location of the YEPPP! library. The program will proceed assuming that the data directory for frame 001 is located at ../../../testdata/2d_0.25/001 and contain subdirectories with input data for H1, L1 and/or V1 detectors (all available detectors are used by default; to select specific detectors, use -usedet option), the grid of parameters files is expected to be in ../../../testdata/2d_0.25/001 , band equals to 1234 1234 , the sampling time dt equals 2 s 2 s , number of days nod in 2 2 , the -addsig option is used to add a software injection to pure noise Gaussian data. The signal's parameters are randomly generated using the sigen code (for more details see the minimal example ). the threshold for the \\mathcal{F} \\mathcal{F} -statistic is set to be 14.5 14.5 , -output is the current directory, --nocheckpoint disables the checkpointing (writing the last visited position on the grid to the state file),","title":"Network of detectors"},{"location":"search_for_candidates/#output-files","text":"Binary output files, containing trigger candidate events above an arbitrary threshold (option -threshold for the \\mathcal{F} \\mathcal{F} -statistic, default 20), are written to the output_dir directory. There are two output files for every input data sequence: triggers_nnn_bbbb_1.bin and triggers_nnn_bbbb_2.bin , where 1 and 2 correspond to the northern and southern ecliptic hemisphere. Each trigger (candidate) event occupies 40 consecutive bytes (5 double numbers), with the following meaning: Record no. | --------------------- | ---------------------------- 1 | frequency [radians, between 0 and \\pi \\pi ] above fpo 2 | spindown [ \\mathrm{Hz/s} \\mathrm{Hz/s} ] 3 | declination [radians, between \\pi/2 \\pi/2 and -\\pi/2 -\\pi/2 ] 4 | right ascension [radians, between 0 and 2\\pi 2\\pi ] 5 | signal-to-noise ratio For the example above, the first 10 triggers from triggers_001_1234_2.bin are 3.05617018e+00 -3.42376198e-08 -7.68007347e-02 2.59248668e+00 5.06667333e+00 1.18243015e+00 -3.20762991e-08 -7.68007347e-02 2.59248668e+00 5.05528873e+00 1.08103361e-01 -2.77536578e-08 -7.68007347e-02 2.59248668e+00 5.07085254e+00 1.90022435e+00 -2.77536578e-08 -7.68007347e-02 2.59248668e+00 5.15191593e+00 1.90000217e+00 -2.55923371e-08 -7.68007347e-02 2.59248668e+00 5.42638039e+00 2.09224664e+00 -2.34310165e-08 -7.68007347e-02 2.59248668e+00 5.20879551e+00 2.38731576e+00 -2.12696958e-08 -7.68007347e-02 2.59248668e+00 5.31983396e+00 3.00543165e+00 -1.91083751e-08 -7.68007347e-02 2.59248668e+00 5.29454616e+00 7.49333983e-01 -1.26244131e-08 -7.68007347e-02 2.59248668e+00 5.08724856e+00 2.08710778e-01 3.43510887e-10 -7.68007347e-02 2.59248668e+00 5.17537018e+00","title":"Output files"},{"location":"search_for_candidates/#auxiliary-output-files","text":"wisdom-hostname.dat - performance-testing file created by the FFTW . The hostname variable is determined by a call to gethostname() , state_nnn_bbbb.dat - checkpointing file containing the last grid point visited. The search can be safely restarted, calculations will continue from the last grid position saved to this file. After successful termination, checkpoint file is left empty.","title":"Auxiliary output files"},{"location":"sensitivity_upper_limits/","text":"Sensitivity upper limits This directory contains a set of scripts that prepare and run a pipeline search in a small area in the parameter space around the injected signal. Prerequisites The scripts require python3 . A working solution is to install a python virtual environment ( python3 comes with a built-in pyvenv virtual environment software). Install python3.4.5 locally Let's assume the installation directory is installdir=/path/to/installdir then mkdir -p ${installdir}; cd ${installdir} wget https://www.python.org/ftp/python/3.4.5/Python-3.4.5.tgz tar zxvf Python-3.4.5.tgz cd Python-3.4.5 make clean ./configure --prefix=$(dirname \"${PWD}\") make -j4 make install cd ../; rm -fr Python-3.4.5* Create virtual environment In a selected location ( /path/to/venvdir ) type ${installdir}/bin/pyvenv venv Activate the virtual environment . /path/to/venvdir/bin/activate (to leave the environment, type deactivate ). You can now install specific packages using the pip installer: pip install numpy pip install scipy pip install matplotlib pip install pandas Running the scripts The steps of the procedure is as follows: Chose the GW strain amplitude h_0 h_0 , Randomly chose other signal parameters (with signal generator sigen ) Add signal to the data (with the gwdetect-cpu --addsig feature) to selected time segments, and perform the search for candidates in each of them ( gwdetect-cpu ), Search for coincidences ( coincidences ) Find if the signal was detected (find the highest coincidences for a given band and compare them with the number of time segments analyzed). The script script.py creates a subdirectory in which the pipeline will be launched based on the following input files: 1. config.ini which contains the paths to codes and the input data, and the parameters of the search: * F-statistic threshold, * how many simulations, * which detectors to use, * size of the region to search, * how to perform the search for coincidences etc. bandlist which is a list of bands with strain amplitudes, for example: 0164 2.25e-1 0165 1.5e-1 2e-1 2.5e-1 3e-1 0166 2e-1 4e-1 The call is % python script.py config.ini bandlist Two other auxiliary files are: 1. Dummy bash script dummy.sh with the actual pipeline calls (variables replaced with actual values by script.py and renamed to script.sh ), 2. PBS/Torque script job.sub , launched into the cluster queue and running script.sh (modify it to fit other systems, e.g., the slurm scheduler). Script script.py creates a run.sh file which contains commands to send the jobs into the queue. The results are summary files ( .sum ) for the requested number of simulations. In order to process them, call the summary.py script % python summary.py band coincidence_threshold number_of_simulations for example % python summary.py 0165 0.7 100 The result will be something as follows (columns are band number, amplitude h , upper limit ul ): band h ul 0165 0.150 0.61 0165 0.200 0.78 0165 0.250 0.95 0165 0.300 0.99 Serial (stacked) version for longer jobs script2.py creates subdirectories and a job_BAND.sub file for a list of amplitudes for BAND from bandlist , stacked one after another (can be handy to send one band as one job to the queue). Call: % python script2.py config.ini bandlist and then (for e.g., band 0165) send it to the queue % qsub -N 0165 -v howmany=100 job_0165.sub The summary of simulations for a given band ( 0165 , say) processed by the summary.sh result in the following list of h0 amplitudes followed by the corresponding fractions of significant coincidences ( N_coin/N ): band h ul 0165 0.150 0.61 0165 0.200 0.78 0165 0.250 0.95 0165 0.300 0.99 We are interested in a 95% upper limit i.e. the h0 corresponding to the fraction 0.95 of significant coincidences in the simulation ( N_coin/N=0.95 ). This is obtained by fitting a sigmoid function def sigmoid(x, x0, k): y = 1.0 / (1.0 + np.exp(k*(x0-x))) return y to the above data. Fitting is done by ul.py : % python ul.py 0165_results 0165 0.01 test.pdf 0165 2.7026e-01 The output is the band number and h0 corresponding to the 95% upper limit. Last command-line option test.pdf is optional. It produces the auxiliary plot, with the 95% upper limit is denoted by red circle:","title":"Sensitivity upper limits"},{"location":"sensitivity_upper_limits/#sensitivity-upper-limits","text":"This directory contains a set of scripts that prepare and run a pipeline search in a small area in the parameter space around the injected signal.","title":"Sensitivity upper limits"},{"location":"sensitivity_upper_limits/#prerequisites","text":"The scripts require python3 . A working solution is to install a python virtual environment ( python3 comes with a built-in pyvenv virtual environment software).","title":"Prerequisites"},{"location":"sensitivity_upper_limits/#install-python345-locally","text":"Let's assume the installation directory is installdir=/path/to/installdir then mkdir -p ${installdir}; cd ${installdir} wget https://www.python.org/ftp/python/3.4.5/Python-3.4.5.tgz tar zxvf Python-3.4.5.tgz cd Python-3.4.5 make clean ./configure --prefix=$(dirname \"${PWD}\") make -j4 make install cd ../; rm -fr Python-3.4.5*","title":"Install python3.4.5 locally"},{"location":"sensitivity_upper_limits/#create-virtual-environment","text":"In a selected location ( /path/to/venvdir ) type ${installdir}/bin/pyvenv venv Activate the virtual environment . /path/to/venvdir/bin/activate (to leave the environment, type deactivate ). You can now install specific packages using the pip installer: pip install numpy pip install scipy pip install matplotlib pip install pandas","title":"Create virtual environment"},{"location":"sensitivity_upper_limits/#running-the-scripts","text":"The steps of the procedure is as follows: Chose the GW strain amplitude h_0 h_0 , Randomly chose other signal parameters (with signal generator sigen ) Add signal to the data (with the gwdetect-cpu --addsig feature) to selected time segments, and perform the search for candidates in each of them ( gwdetect-cpu ), Search for coincidences ( coincidences ) Find if the signal was detected (find the highest coincidences for a given band and compare them with the number of time segments analyzed). The script script.py creates a subdirectory in which the pipeline will be launched based on the following input files: 1. config.ini which contains the paths to codes and the input data, and the parameters of the search: * F-statistic threshold, * how many simulations, * which detectors to use, * size of the region to search, * how to perform the search for coincidences etc. bandlist which is a list of bands with strain amplitudes, for example: 0164 2.25e-1 0165 1.5e-1 2e-1 2.5e-1 3e-1 0166 2e-1 4e-1 The call is % python script.py config.ini bandlist Two other auxiliary files are: 1. Dummy bash script dummy.sh with the actual pipeline calls (variables replaced with actual values by script.py and renamed to script.sh ), 2. PBS/Torque script job.sub , launched into the cluster queue and running script.sh (modify it to fit other systems, e.g., the slurm scheduler). Script script.py creates a run.sh file which contains commands to send the jobs into the queue. The results are summary files ( .sum ) for the requested number of simulations. In order to process them, call the summary.py script % python summary.py band coincidence_threshold number_of_simulations for example % python summary.py 0165 0.7 100 The result will be something as follows (columns are band number, amplitude h , upper limit ul ): band h ul 0165 0.150 0.61 0165 0.200 0.78 0165 0.250 0.95 0165 0.300 0.99","title":"Running the scripts"},{"location":"sensitivity_upper_limits/#serial-stacked-version-for-longer-jobs","text":"script2.py creates subdirectories and a job_BAND.sub file for a list of amplitudes for BAND from bandlist , stacked one after another (can be handy to send one band as one job to the queue). Call: % python script2.py config.ini bandlist and then (for e.g., band 0165) send it to the queue % qsub -N 0165 -v howmany=100 job_0165.sub The summary of simulations for a given band ( 0165 , say) processed by the summary.sh result in the following list of h0 amplitudes followed by the corresponding fractions of significant coincidences ( N_coin/N ): band h ul 0165 0.150 0.61 0165 0.200 0.78 0165 0.250 0.95 0165 0.300 0.99 We are interested in a 95% upper limit i.e. the h0 corresponding to the fraction 0.95 of significant coincidences in the simulation ( N_coin/N=0.95 ). This is obtained by fitting a sigmoid function def sigmoid(x, x0, k): y = 1.0 / (1.0 + np.exp(k*(x0-x))) return y to the above data. Fitting is done by ul.py : % python ul.py 0165_results 0165 0.01 test.pdf 0165 2.7026e-01 The output is the band number and h0 corresponding to the 95% upper limit. Last command-line option test.pdf is optional. It produces the auxiliary plot, with the 95% upper limit is denoted by red circle:","title":"Serial (stacked) version for longer jobs"},{"location":"simple_example/","text":"Pipeline: a minimal example This is a demonstration of the pipeline using Gaussian noise test data with randomly-selected signal added to the data (software injection). Structure of the input data The generic directory structure of the input data is 001 \u251c\u2500\u2500 grid.bin \u251c\u2500\u2500 H1 \u2502 \u251c\u2500\u2500 DetSSB.bin \u2502 \u251c\u2500\u2500 grid.bin \u2502 \u251c\u2500\u2500 rDet.bin \u2502 \u251c\u2500\u2500 rSSB.bin \u2502 \u251c\u2500\u2500 starting_date \u2502 \u2514\u2500\u2500 xdatc_001_1234.bin \u2514\u2500\u2500 L1 \u251c\u2500\u2500 DetSSB.bin \u251c\u2500\u2500 grid.bin \u251c\u2500\u2500 rDet.bin \u251c\u2500\u2500 rSSB.bin \u251c\u2500\u2500 starting_date \u2514\u2500\u2500 xdatc_001_1234.bin (here for two LIGO detectors H1 and L1, and frame 001 ). Test data frames nnn=001-008 nnn=001-008 with pure Gaussian noise 2-day time segments with sampling time equal to 2s ( xdatc_nnn_1234.bin ) are available here . In principle, given the ephemerides ( DetSSB.bin , rDet.bin and rSSB.bin ) for each detector and frame 001-008 , one can create the grid matrices using the gridgen implementation (see the code for details): # grid generation cd gridgen make for ifo in H1 L1; do for d in $(seq -f %03g 1 8); do ./gridgen -m 0.5 -p dfg -d ../testdata/2d_0.25/${d}/${ifo}/ -n 17; done; done # copying the H1 grid file one level up for the case of the network search for d in $(seq -f %03g 1 8); do cp -v ../testdata/2d_0.25/${d}/H1/grid.bin ../testdata/2d_0.25/${d}; done Test Gaussian noise time series data were created as follows: #!/bin/bash band=1234 # Gaussian data generation cd ../search/network/scr-cpu gcc gauss-xdat.c -o gauss-xdat -lm -lgsl -lgslcblas # 86164: number of points in 2-day segment with 2s sampling time for ifo in H1 L1; do for d in $(seq -f %03g 1 8); do echo $d $ifo; ./gauss-xdat 86164 1 1 ../../../testdata/2d_0.25/${d}/${ifo}/xdatc_${d}_${band}.bin; done; done Given the complete input data, this pipeline minimal example consists of Adding an artificial signal to the data (random parameters of the signal generated with sigen ), Performing a search around the injection for each time segment ( gwsearch-cpu ), Looking for coincidences between the candidate signals from different time frames ( coincidences ), Establishing the false alarm probability of the best coincidence ( fap ). Generating random parameters for the signal Random parameters of the signal are chosen using the sigen and added to the data time series with the add_signal() function in ( init ). # Create random parameters of a signal signal cd search/network/src-cpu make sigen band=1234; dt=2; nod=2; ./sigen -amp 4.e-2 -band $band -dt $dt -gsize 10 -reffr 4 -nod $nod 1> sig1 Signal parameters used in this example: % cat sig1 amp 4.000000e-02 10 4 9.9791082090028898e-01 -1.6533871297433800e-09 -1.1821269273133420e-01 1.9839903273071489e+00 4.7717937494571394e-01 7.5715524886052021e-01 7.5154297884129850e-01 -4.7938541489358644e-01 Adding signal to the Gaussian data and searching for candidates band=1234; dt=2; nod=2; for d in $(seq -f %03g 1 8); do LD_LIBRARY_PATH=lib/yeppp-1.0.0/binaries/linux/x86_64 ./gwsearch-cpu \\ -data ../../../testdata/2d_0.25/ \\ -ident ${d} \\ -band $band \\ -dt $dt \\ -nod $nod \\ -addsig sig1 \\ -output . \\ -threshold 14.5 \\ --nocheckpoint \\ done This produces trigger files for each frame (size in bytes also listed): 99320 triggers_001_1234_2.bin 89960 triggers_002_1234_2.bin 89880 triggers_003_1234_2.bin 95360 triggers_004_1234_2.bin 81600 triggers_005_1234_2.bin 92200 triggers_006_1234_2.bin 89040 triggers_007_1234_2.bin 96320 triggers_008_1234_2.bin First 10 triggers from triggers_001_1234_2.bin are 3.05617018e+00 -3.42376198e-08 -7.68007347e-02 2.59248668e+00 5.06667333e+00 1.18243015e+00 -3.20762991e-08 -7.68007347e-02 2.59248668e+00 5.05528873e+00 1.08103361e-01 -2.77536578e-08 -7.68007347e-02 2.59248668e+00 5.07085254e+00 1.90022435e+00 -2.77536578e-08 -7.68007347e-02 2.59248668e+00 5.15191593e+00 1.90000217e+00 -2.55923371e-08 -7.68007347e-02 2.59248668e+00 5.42638039e+00 2.09224664e+00 -2.34310165e-08 -7.68007347e-02 2.59248668e+00 5.20879551e+00 2.38731576e+00 -2.12696958e-08 -7.68007347e-02 2.59248668e+00 5.31983396e+00 3.00543165e+00 -1.91083751e-08 -7.68007347e-02 2.59248668e+00 5.29454616e+00 7.49333983e-01 -1.26244131e-08 -7.68007347e-02 2.59248668e+00 5.08724856e+00 2.08710778e-01 3.43510887e-10 -7.68007347e-02 2.59248668e+00 5.17537018e+00 Coincidences among these trigger files cd ../../../coincidences/src make band=1234; dt=2; nod=2; fpo=$(echo $band $dt |awk '{printf(\"%.6f\", 10 + 0.96875*$1/(2.0*$2))}'); for s in {0..1}{0..1}{0..1}{0..1}; do ./coincidences \\ -data ../../search/network/src-cpu \\ -output . \\ -shift $s \\ -scalef 4 \\ -scales 4 \\ -scaled 4 \\ -scalea 4 \\ -refr 4 \\ -dt $dt \\ -trigname ${band}_2 \\ -refloc ../../testdata/2d_0.25/004 \\ -nod $nod \\ -fpo $fpo \\ -snrcutoff 5 \\ done 2>> summary # best shift (highest multiplicity with largest snr) sort -gk5 -gk10 summary | tail -1 The highest coincidence with the largest signal-to-noise ratio is 1234_2 1111 308.859375 8 5 9.95663703e-01 -1.10830358e-09 -1.12585347e-01 1.97463002e+00 1.246469e+01 5 2040 1987 1 2483 2419 4 2384 2193 3 2247 2137 8 2408 2363 2 2249 2172 6 2305 2220 7 2226 2191 6 2 8 3 5 False alarm probability make fap fap.sh <(sort -gk5 -gk10 summary | tail -1) <(echo $band 0.0) ../../testdata/2d_0.25/004 resulting in Number of days in time segments: 2 Input data: /dev/fd/63 Grid matrix data directory: ../../testdata/2d_0.25/004 Band number: 1234 (veto fraction: 0.000000) The reference frequency fpo: 308.859375 The data sampling time dt: 2.000000 FAP threshold: 1.000000 Cell size: 4 1234 3.088594e+02 3.091094e+02 7.665713e-08 5 17682 9.956637e-01 -1.108304e-09 -1.125853e-01 1.974630e+00 1.246469e+01 2 The false alarm probability in this case is 7.665713e-08 . It's low enough to be an interesting outlier for a followup procedure.","title":"Pipeline: a minimal example"},{"location":"simple_example/#pipeline-a-minimal-example","text":"This is a demonstration of the pipeline using Gaussian noise test data with randomly-selected signal added to the data (software injection).","title":"Pipeline: a minimal example"},{"location":"simple_example/#structure-of-the-input-data","text":"The generic directory structure of the input data is 001 \u251c\u2500\u2500 grid.bin \u251c\u2500\u2500 H1 \u2502 \u251c\u2500\u2500 DetSSB.bin \u2502 \u251c\u2500\u2500 grid.bin \u2502 \u251c\u2500\u2500 rDet.bin \u2502 \u251c\u2500\u2500 rSSB.bin \u2502 \u251c\u2500\u2500 starting_date \u2502 \u2514\u2500\u2500 xdatc_001_1234.bin \u2514\u2500\u2500 L1 \u251c\u2500\u2500 DetSSB.bin \u251c\u2500\u2500 grid.bin \u251c\u2500\u2500 rDet.bin \u251c\u2500\u2500 rSSB.bin \u251c\u2500\u2500 starting_date \u2514\u2500\u2500 xdatc_001_1234.bin (here for two LIGO detectors H1 and L1, and frame 001 ). Test data frames nnn=001-008 nnn=001-008 with pure Gaussian noise 2-day time segments with sampling time equal to 2s ( xdatc_nnn_1234.bin ) are available here . In principle, given the ephemerides ( DetSSB.bin , rDet.bin and rSSB.bin ) for each detector and frame 001-008 , one can create the grid matrices using the gridgen implementation (see the code for details): # grid generation cd gridgen make for ifo in H1 L1; do for d in $(seq -f %03g 1 8); do ./gridgen -m 0.5 -p dfg -d ../testdata/2d_0.25/${d}/${ifo}/ -n 17; done; done # copying the H1 grid file one level up for the case of the network search for d in $(seq -f %03g 1 8); do cp -v ../testdata/2d_0.25/${d}/H1/grid.bin ../testdata/2d_0.25/${d}; done Test Gaussian noise time series data were created as follows: #!/bin/bash band=1234 # Gaussian data generation cd ../search/network/scr-cpu gcc gauss-xdat.c -o gauss-xdat -lm -lgsl -lgslcblas # 86164: number of points in 2-day segment with 2s sampling time for ifo in H1 L1; do for d in $(seq -f %03g 1 8); do echo $d $ifo; ./gauss-xdat 86164 1 1 ../../../testdata/2d_0.25/${d}/${ifo}/xdatc_${d}_${band}.bin; done; done Given the complete input data, this pipeline minimal example consists of Adding an artificial signal to the data (random parameters of the signal generated with sigen ), Performing a search around the injection for each time segment ( gwsearch-cpu ), Looking for coincidences between the candidate signals from different time frames ( coincidences ), Establishing the false alarm probability of the best coincidence ( fap ).","title":"Structure of the input data"},{"location":"simple_example/#generating-random-parameters-for-the-signal","text":"Random parameters of the signal are chosen using the sigen and added to the data time series with the add_signal() function in ( init ). # Create random parameters of a signal signal cd search/network/src-cpu make sigen band=1234; dt=2; nod=2; ./sigen -amp 4.e-2 -band $band -dt $dt -gsize 10 -reffr 4 -nod $nod 1> sig1 Signal parameters used in this example: % cat sig1 amp 4.000000e-02 10 4 9.9791082090028898e-01 -1.6533871297433800e-09 -1.1821269273133420e-01 1.9839903273071489e+00 4.7717937494571394e-01 7.5715524886052021e-01 7.5154297884129850e-01 -4.7938541489358644e-01","title":"Generating random parameters for the signal"},{"location":"simple_example/#adding-signal-to-the-gaussian-data-and-searching-for-candidates","text":"band=1234; dt=2; nod=2; for d in $(seq -f %03g 1 8); do LD_LIBRARY_PATH=lib/yeppp-1.0.0/binaries/linux/x86_64 ./gwsearch-cpu \\ -data ../../../testdata/2d_0.25/ \\ -ident ${d} \\ -band $band \\ -dt $dt \\ -nod $nod \\ -addsig sig1 \\ -output . \\ -threshold 14.5 \\ --nocheckpoint \\ done This produces trigger files for each frame (size in bytes also listed): 99320 triggers_001_1234_2.bin 89960 triggers_002_1234_2.bin 89880 triggers_003_1234_2.bin 95360 triggers_004_1234_2.bin 81600 triggers_005_1234_2.bin 92200 triggers_006_1234_2.bin 89040 triggers_007_1234_2.bin 96320 triggers_008_1234_2.bin First 10 triggers from triggers_001_1234_2.bin are 3.05617018e+00 -3.42376198e-08 -7.68007347e-02 2.59248668e+00 5.06667333e+00 1.18243015e+00 -3.20762991e-08 -7.68007347e-02 2.59248668e+00 5.05528873e+00 1.08103361e-01 -2.77536578e-08 -7.68007347e-02 2.59248668e+00 5.07085254e+00 1.90022435e+00 -2.77536578e-08 -7.68007347e-02 2.59248668e+00 5.15191593e+00 1.90000217e+00 -2.55923371e-08 -7.68007347e-02 2.59248668e+00 5.42638039e+00 2.09224664e+00 -2.34310165e-08 -7.68007347e-02 2.59248668e+00 5.20879551e+00 2.38731576e+00 -2.12696958e-08 -7.68007347e-02 2.59248668e+00 5.31983396e+00 3.00543165e+00 -1.91083751e-08 -7.68007347e-02 2.59248668e+00 5.29454616e+00 7.49333983e-01 -1.26244131e-08 -7.68007347e-02 2.59248668e+00 5.08724856e+00 2.08710778e-01 3.43510887e-10 -7.68007347e-02 2.59248668e+00 5.17537018e+00","title":"Adding signal to the Gaussian data and searching for candidates"},{"location":"simple_example/#coincidences-among-these-trigger-files","text":"cd ../../../coincidences/src make band=1234; dt=2; nod=2; fpo=$(echo $band $dt |awk '{printf(\"%.6f\", 10 + 0.96875*$1/(2.0*$2))}'); for s in {0..1}{0..1}{0..1}{0..1}; do ./coincidences \\ -data ../../search/network/src-cpu \\ -output . \\ -shift $s \\ -scalef 4 \\ -scales 4 \\ -scaled 4 \\ -scalea 4 \\ -refr 4 \\ -dt $dt \\ -trigname ${band}_2 \\ -refloc ../../testdata/2d_0.25/004 \\ -nod $nod \\ -fpo $fpo \\ -snrcutoff 5 \\ done 2>> summary # best shift (highest multiplicity with largest snr) sort -gk5 -gk10 summary | tail -1 The highest coincidence with the largest signal-to-noise ratio is 1234_2 1111 308.859375 8 5 9.95663703e-01 -1.10830358e-09 -1.12585347e-01 1.97463002e+00 1.246469e+01 5 2040 1987 1 2483 2419 4 2384 2193 3 2247 2137 8 2408 2363 2 2249 2172 6 2305 2220 7 2226 2191 6 2 8 3 5","title":"Coincidences among these trigger files"},{"location":"simple_example/#false-alarm-probability","text":"make fap fap.sh <(sort -gk5 -gk10 summary | tail -1) <(echo $band 0.0) ../../testdata/2d_0.25/004 resulting in Number of days in time segments: 2 Input data: /dev/fd/63 Grid matrix data directory: ../../testdata/2d_0.25/004 Band number: 1234 (veto fraction: 0.000000) The reference frequency fpo: 308.859375 The data sampling time dt: 2.000000 FAP threshold: 1.000000 Cell size: 4 1234 3.088594e+02 3.091094e+02 7.665713e-08 5 17682 9.956637e-01 -1.108304e-09 -1.125853e-01 1.974630e+00 1.246469e+01 2 The false alarm probability in this case is 7.665713e-08 . It's low enough to be an interesting outlier for a followup procedure.","title":"False alarm probability"},{"location":"Docs_howto/configuration/","text":"Docs Configuration This documentation is written in Markdown. We use the standard Github Pages pipeline to generate viewable web pages. The Github pipeline, by default uses Jekyll static site generator written in Rubby. TDFstat project on Github is configured to use the /docs directory in the master branch as source to this pipeline. In file _config.yml we use an external Read The Docs Theme for Jekyll and GitHub Pages .\\ We customized the default theme's file _layouts/base.html as follows: - added support for mathjax (with $ as delimiters) <script> MathJax = { tex: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], /*processEscapes: false,*/ } }; </script> <script type=\"text/javascript\" id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"> </script> changed the default RTD theme width from 800 px to 100% ( why is it hardcoded in RTD? ) <style> .wy-nav-content { max-width: none; } </style> There is also the Gemfile file. It is the same as the default one used by github internally to build the site with jekyll. We add it here explicitelly becuse it is needed to build the site locally. Usefull documentation Jekyll Github pages Theme documentation","title":"Docs configuration"},{"location":"Docs_howto/configuration/#docs-configuration","text":"This documentation is written in Markdown. We use the standard Github Pages pipeline to generate viewable web pages. The Github pipeline, by default uses Jekyll static site generator written in Rubby. TDFstat project on Github is configured to use the /docs directory in the master branch as source to this pipeline. In file _config.yml we use an external Read The Docs Theme for Jekyll and GitHub Pages .\\ We customized the default theme's file _layouts/base.html as follows: - added support for mathjax (with $ as delimiters) <script> MathJax = { tex: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], /*processEscapes: false,*/ } }; </script> <script type=\"text/javascript\" id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"> </script> changed the default RTD theme width from 800 px to 100% ( why is it hardcoded in RTD? ) <style> .wy-nav-content { max-width: none; } </style> There is also the Gemfile file. It is the same as the default one used by github internally to build the site with jekyll. We add it here explicitelly becuse it is needed to build the site locally.","title":"Docs Configuration"},{"location":"Docs_howto/configuration/#usefull-documentation","text":"Jekyll Github pages Theme documentation","title":"Usefull documentation"},{"location":"Docs_howto/syntax/","text":"Markdown syntax documentation Jekyll uses Kramdown which is a superset of Markdown. For math support see MathJax documentation . Please note that MathJax supports only the math-mode commands from Latex , we have enabled single $ as math delimiter","title":"Markdown syntax"},{"location":"Docs_howto/syntax/#markdown-syntax-documentation","text":"Jekyll uses Kramdown which is a superset of Markdown. For math support see MathJax documentation . Please note that MathJax supports only the math-mode commands from Latex , we have enabled single $ as math delimiter","title":"Markdown syntax documentation"},{"location":"Docs_howto/testing/","text":"Testing the documentation locally Before commiting any changes to github we should test them locally. We'll need ruby and many required gems (including jekyll and github-pages) installed in a virtual environment using conda. conda create -n jekyll-test conda activate jekyll-test # apparently ruby packages at conda-forge are messed, this should work # conda install ruby rb-bundler # but in my case I needed conda install ruby rb-bundler c-compiler compilers cxx-compiler # now change to the directory with documentation: `<somedir>/TDFstat/docs` and run bundle install # (this will install all the gems) Having the virtual environment ready it is very easy to view the documentation locally. Just change to <somedir>/TDFstat/docs directory and run: bundle exec jekyll serve -l This command generates static pages with jekyll and starts local web server hosting them at address listed in the output, usually http://127.0.0.1:4000 . Open this location in any web browser. \\ To facilitate editing, if any file in the docs directory is modified the pages will automatically rebuild and reload in the browser. The static web site is output to the _site subdirectory. Do not commit it to the repository - we keep there only sources and neccessary configurations.","title":"Local testing"},{"location":"Docs_howto/testing/#testing-the-documentation-locally","text":"Before commiting any changes to github we should test them locally. We'll need ruby and many required gems (including jekyll and github-pages) installed in a virtual environment using conda. conda create -n jekyll-test conda activate jekyll-test # apparently ruby packages at conda-forge are messed, this should work # conda install ruby rb-bundler # but in my case I needed conda install ruby rb-bundler c-compiler compilers cxx-compiler # now change to the directory with documentation: `<somedir>/TDFstat/docs` and run bundle install # (this will install all the gems) Having the virtual environment ready it is very easy to view the documentation locally. Just change to <somedir>/TDFstat/docs directory and run: bundle exec jekyll serve -l This command generates static pages with jekyll and starts local web server hosting them at address listed in the output, usually http://127.0.0.1:4000 . Open this location in any web browser. \\ To facilitate editing, if any file in the docs directory is modified the pages will automatically rebuild and reload in the browser. The static web site is output to the _site subdirectory. Do not commit it to the repository - we keep there only sources and neccessary configurations.","title":"Testing the documentation locally"},{"location":"mkdocs_howto/","text":"Documentation How To Introduction This documentation is written in Markdown. We use the MkDocs static site generator (written in python) to create html pages. The documentation sources (markdown) reside in the project's docs subdirectory (this is the mkdocs default). Whenever you modify contents of this directory (and push changes to Github), Github runs our custom workflow to regenerate html pages and stores them in the gh-pages branch which is published at https://polgraw.github.io/TDFstat/ . Writing documentation Some usefull documentation for authors: Markdown MkDocs documentation for users Math formulas ( $ for inline and $$ for block, begin{equation} , etc...) Code highlighting (e.g. ```python ) Admonitions ( !!! note|danger|important|highlight|blink ) Please test any changes locally before pushing them to Github! MkDocs configuration The main configuration file for MkDocs, mkdocs.yml , resides in the project's root directory. The nav section defines site menu. In the theme section we enabled Read the docs theme . RTD theme is modified using CSS file 'extra.css' and we change there the default width of content from 800 px to 100%. Why the width is hardcoded in RTD? . In addition we configure highlight.js package used by default in RTD. In particullar we load custom JS code from javascripts/highlight.js . This is the only way I have found to disable code highlighting by default in highlight.js, otherwise you get random languages detected on simple text. With this modification we have to specify the language explicitelly to enable highlighting (e.g. ```python ). Only common languages are loaded by default , if you need some other language add it to the list in mkdocs.yml . We use MathJax along with Artihmatex extension (to preserve latex syntax with single $). The whole configuration is build around new MathJax 3.x. Following guidelines from Artihmatex documentation we have: generic option is set to false we load tex-chtml combined components (for other options see mathjax components docs ) MathJax 3.x configuration is loaded via docs/javascripts/mathjax.js file Note There is no need to load MathJax explicitelly in mkdocs.yml since Arithmatex does this Important docs/javascripts/mathjax.js must be loaded before MathJax ( - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js ) Testing locally and pushing to Github You should install MkDocs in the conda virtual environment (with conda-forge enabled): conda create -n mkdocs-test conda activate mkdocs-test conda install mkdocs pymdown-extensions pygments Then go to the root directory of the repository, <some dir>/TDFstat and run mkdocs serve -v This should start a local webserver at address http://127.0.0.1:8000/ . Check it in the web browser. The pages will regenerate and reload autonatically if there are any changes. After testing if the pages are working locally you can submit changes to master branch git add <some files> git commit -a -m \"[docs] something\" git push origin main Now you can go to the repository actions and observe two build actions being executed. The first one is building mkdocs site and pushing output to gh-pages branch. This action is defined in this yml file . Alternativelly, you can achive the same manually from your local testing environment: mkdocs gh-deploy The second action does deployment of html code to https://polgraw.github.io/TDFstat . Possible improvements: https://datamattsson.tumblr.com/post/612351271067893760/the-perfect-documentation-storm https://github.com/marketplace/actions/deploy-mkdocs","title":"Docs howto"},{"location":"mkdocs_howto/#documentation-how-to","text":"","title":"Documentation How To"},{"location":"mkdocs_howto/#introduction","text":"This documentation is written in Markdown. We use the MkDocs static site generator (written in python) to create html pages. The documentation sources (markdown) reside in the project's docs subdirectory (this is the mkdocs default). Whenever you modify contents of this directory (and push changes to Github), Github runs our custom workflow to regenerate html pages and stores them in the gh-pages branch which is published at https://polgraw.github.io/TDFstat/ .","title":"Introduction"},{"location":"mkdocs_howto/#writing-documentation","text":"Some usefull documentation for authors: Markdown MkDocs documentation for users Math formulas ( $ for inline and $$ for block, begin{equation} , etc...) Code highlighting (e.g. ```python ) Admonitions ( !!! note|danger|important|highlight|blink ) Please test any changes locally before pushing them to Github!","title":"Writing documentation"},{"location":"mkdocs_howto/#mkdocs-configuration","text":"The main configuration file for MkDocs, mkdocs.yml , resides in the project's root directory. The nav section defines site menu. In the theme section we enabled Read the docs theme . RTD theme is modified using CSS file 'extra.css' and we change there the default width of content from 800 px to 100%. Why the width is hardcoded in RTD? . In addition we configure highlight.js package used by default in RTD. In particullar we load custom JS code from javascripts/highlight.js . This is the only way I have found to disable code highlighting by default in highlight.js, otherwise you get random languages detected on simple text. With this modification we have to specify the language explicitelly to enable highlighting (e.g. ```python ). Only common languages are loaded by default , if you need some other language add it to the list in mkdocs.yml . We use MathJax along with Artihmatex extension (to preserve latex syntax with single $). The whole configuration is build around new MathJax 3.x. Following guidelines from Artihmatex documentation we have: generic option is set to false we load tex-chtml combined components (for other options see mathjax components docs ) MathJax 3.x configuration is loaded via docs/javascripts/mathjax.js file Note There is no need to load MathJax explicitelly in mkdocs.yml since Arithmatex does this Important docs/javascripts/mathjax.js must be loaded before MathJax ( - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js )","title":"MkDocs configuration"},{"location":"mkdocs_howto/#testing","text":"You should install MkDocs in the conda virtual environment (with conda-forge enabled): conda create -n mkdocs-test conda activate mkdocs-test conda install mkdocs pymdown-extensions pygments Then go to the root directory of the repository, <some dir>/TDFstat and run mkdocs serve -v This should start a local webserver at address http://127.0.0.1:8000/ . Check it in the web browser. The pages will regenerate and reload autonatically if there are any changes. After testing if the pages are working locally you can submit changes to master branch git add <some files> git commit -a -m \"[docs] something\" git push origin main Now you can go to the repository actions and observe two build actions being executed. The first one is building mkdocs site and pushing output to gh-pages branch. This action is defined in this yml file . Alternativelly, you can achive the same manually from your local testing environment: mkdocs gh-deploy The second action does deployment of html code to https://polgraw.github.io/TDFstat . Possible improvements: https://datamattsson.tumblr.com/post/612351271067893760/the-perfect-documentation-storm https://github.com/marketplace/actions/deploy-mkdocs","title":"Testing locally and pushing to Github"}]}